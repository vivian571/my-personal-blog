     # 第15节：多任务爬虫

## 学习目标

- **<font color="red">理解进程与线程的基本概念</font>**
- **<font color="blue">掌握Python多线程的创建方法</font>**
- **<font color="green">学习线程间通信与数据共享</font>**
- **<font color="purple">实现多线程爬虫案例</font>**
- **<font color="orange">了解GIL对Python多线程的影响</font>**

## 知识点

### 进程与线程

- **<font color="red">进程定义</font>**：
  - 操作系统分配资源的基本单位
  - 独立的内存空间和系统资源
  - 相互隔离，安全性高

- **<font color="blue">线程定义</font>**：
  - 进程内的执行单元
  - 共享所属进程的内存空间
  - 创建和切换开销小

- **<font color="green">区别与联系</font>**：
  - 进程是资源分配的单位，线程是CPU调度的单位
  - 一个进程可以包含多个线程
  - 线程之间共享进程资源，进程之间相互独立
  - 线程切换开销小，进程切换开销大

### Python多线程

- **<font color="red">threading模块</font>**：
  - Python标准库提供的线程实现
  - Thread类用于创建线程
  - 支持守护线程、线程同步等功能

- **<font color="blue">创建线程的方法</font>**：
  - 直接实例化Thread类
  - 继承Thread类并重写run方法
  - 使用线程池（concurrent.futures）

- **<font color="green">线程状态</font>**：
  - 新建（New）
  - 就绪（Runnable）
  - 运行（Running）
  - 阻塞（Blocked）
  - 死亡（Dead）

### 线程间通信

- **<font color="red">共享变量</font>**：
  - 全局变量
  - 类成员变量
  - 注意线程安全问题

- **<font color="blue">Queue队列</font>**：
  - 线程安全的FIFO队列
  - 支持阻塞操作
  - 适合生产者-消费者模型

- **<font color="green">Event事件</font>**：
  - 用于线程间的通知机制
  - 可以设置和清除标志
  - wait()方法等待标志被设置

### GIL（全局解释器锁）

- **<font color="red">定义</font>**：
  - Python解释器的互斥锁
  - 同一时刻只允许一个线程执行Python字节码

- **<font color="blue">影响</font>**：
  - 限制了多线程对CPU密集型任务的性能提升
  - 对IO密集型任务影响较小
  - 多进程可以绕过GIL限制

- **<font color="green">适用场景</font>**：
  - IO密集型任务（网络请求、文件操作）
  - 不适合CPU密集型计算

## 典型示例

### 创建线程的基本方法

```python
import threading
import time

# 方法1：直接实例化Thread类
def worker_function(name):
    print(f"线程 {name} 开始工作")
    time.sleep(2)  # 模拟工作耗时
    print(f"线程 {name} 工作完成")

# 创建线程
thread1 = threading.Thread(target=worker_function, args=("Thread-1",))
# 启动线程
thread1.start()
# 等待线程结束
thread1.join()

# 方法2：继承Thread类
class WorkerThread(threading.Thread):
    def __init__(self, name):
        super().__init__()
        self.name = name
    
    def run(self):
        print(f"线程 {self.name} 开始工作")
        time.sleep(2)  # 模拟工作耗时
        print(f"线程 {self.name} 工作完成")

# 创建并启动线程
thread2 = WorkerThread("Thread-2")
thread2.start()
thread2.join()

print("所有线程执行完毕")
```

### 使用Queue实现线程通信

```python
import threading
import queue
import time
import random

# 创建队列
work_queue = queue.Queue(maxsize=10)

# 生产者线程
class Producer(threading.Thread):
    def __init__(self, name):
        super().__init__()
        self.name = name
    
    def run(self):
        print(f"生产者 {self.name} 开始生产数据")
        for i in range(5):
            item = f"数据-{i}"
            # 将数据放入队列
            work_queue.put(item)
            print(f"生产者 {self.name} 生产了: {item}")
            time.sleep(random.random())

# 消费者线程
class Consumer(threading.Thread):
    def __init__(self, name):
        super().__init__()
        self.name = name
    
    def run(self):
        print(f"消费者 {self.name} 开始消费数据")
        while True:
            try:
                # 从队列获取数据，设置超时时间
                item = work_queue.get(timeout=3)
                print(f"消费者 {self.name} 消费了: {item}")
                # 标记任务完成
                work_queue.task_done()
                time.sleep(random.random() * 2)
            except queue.Empty:
                print(f"消费者 {self.name} 等待超时，退出")
                break

# 创建并启动生产者和消费者线程
producers = [Producer(f"P-{i}") for i in range(2)]
consumers = [Consumer(f"C-{i}") for i in range(3)]

for p in producers:
    p.start()

for c in consumers:
    c.start()

# 等待所有生产者完成
for p in producers:
    p.join()

# 等待队列中的所有任务被处理
work_queue.join()

print("所有数据处理完毕")
```

### 线程同步与锁

```python
import threading
import time

# 共享资源
counter = 0

# 创建锁
lock = threading.Lock()

def increment_with_lock():
    global counter
    for _ in range(100000):
        # 获取锁
        lock.acquire()
        try:
            # 修改共享资源
            counter += 1
        finally:
            # 释放锁
            lock.release()

def increment_without_lock():
    global counter
    for _ in range(100000):
        # 直接修改共享资源（不安全）
        counter += 1

# 测试使用锁
def test_with_lock():
    global counter
    counter = 0
    threads = []
    
    # 创建5个线程
    for i in range(5):
        t = threading.Thread(target=increment_with_lock)
        threads.append(t)
        t.start()
    
    # 等待所有线程完成
    for t in threads:
        t.join()
    
    print(f"使用锁后结果: {counter}")

# 测试不使用锁
def test_without_lock():
    global counter
    counter = 0
    threads = []
    
    # 创建5个线程
    for i in range(5):
        t = threading.Thread(target=increment_without_lock)
        threads.append(t)
        t.start()
    
    # 等待所有线程完成
    for t in threads:
        t.join()
    
    print(f"不使用锁结果: {counter}")

# 执行测试
test_with_lock()    # 预期输出: 500000
test_without_lock() # 预期输出: 小于500000的值
```

## 实际示例

### 多线程爬虫实现

```python
import requests
import threading
import queue
import time
from bs4 import BeautifulSoup
import os
import random
from urllib.parse import urljoin

# 创建请求头
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# 创建队列
url_queue = queue.Queue()
html_queue = queue.Queue()
img_queue = queue.Queue()

# 创建锁
lock = threading.Lock()

# 计数器
class Counter:
    def __init__(self):
        self.value = 0
        self.lock = threading.Lock()
    
    def increment(self):
        with self.lock:
            self.value += 1
            return self.value
    
    def get(self):
        with self.lock:
            return self.value

# 统计计数器
processed_urls = Counter()
downloaded_images = Counter()

# URL爬取线程
class UrlCrawler(threading.Thread):
    def __init__(self, base_url, max_pages=5):
        super().__init__()
        self.base_url = base_url
        self.max_pages = max_pages
        self.visited_urls = set()
    
    def run(self):
        # 添加起始URL到队列
        url_queue.put(self.base_url)
        self.visited_urls.add(self.base_url)
        
        while len(self.visited_urls) < self.max_pages:
            try:
                # 从队列获取URL
                current_url = url_queue.get(timeout=5)
                
                try:
                    # 发送请求
                    response = requests.get(current_url, headers=headers, timeout=10)
                    if response.status_code == 200:
                        # 将HTML内容放入队列
                        html_queue.put((current_url, response.text))
                        processed_urls.increment()
                        print(f"已爬取页面: {current_url}")
                    
                    # 随机延时，避免请求过快
                    time.sleep(random.uniform(1, 3))
                    
                except Exception as e:
                    print(f"请求出错: {current_url}, 错误: {str(e)}")
                
                finally:
                    # 标记任务完成
                    url_queue.task_done()
                    
            except queue.Empty:
                # 队列为空，退出循环
                break

# HTML解析线程
class HtmlParser(threading.Thread):
    def __init__(self, url_crawler):
        super().__init__()
        self.url_crawler = url_crawler
    
    def run(self):
        while True:
            try:
                # 从队列获取HTML内容
                current_url, html_content = html_queue.get(timeout=5)
                
                try:
                    # 解析HTML
                    soup = BeautifulSoup(html_content, 'html.parser')
                    
                    # 提取图片URL
                    img_tags = soup.find_all('img')
                    for img in img_tags:
                        img_url = img.get('src')
                        if img_url:
                            # 处理相对路径
                            if not img_url.startswith('http'):
                                img_url = urljoin(current_url, img_url)
                            
                            # 将图片URL放入队列
                            img_queue.put(img_url)
                    
                    # 提取链接URL
                    if len(self.url_crawler.visited_urls) < self.url_crawler.max_pages:
                        links = soup.find_all('a')
                        for link in links:
                            href = link.get('href')
                            if href and href.startswith('http'):
                                # 检查是否已访问过
                                if href not in self.url_crawler.visited_urls:
                                    url_queue.put(href)
                                    self.url_crawler.visited_urls.add(href)
                                    # 如果达到最大页面数，退出循环
                                    if len(self.url_crawler.visited_urls) >= self.url_crawler.max_pages:
                                        break
                
                except Exception as e:
                    print(f"解析出错: {current_url}, 错误: {str(e)}")
                
                finally:
                    # 标记任务完成
                    html_queue.task_done()
            
            except queue.Empty:
                # 如果URL爬取线程已结束且队列为空，退出循环
                if not self.url_crawler.is_alive() and html_queue.empty():
                    break
                # 否则继续等待
                continue

# 图片下载线程
class ImageDownloader(threading.Thread):
    def __init__(self, save_dir='downloaded_images'):
        super().__init__()
        self.save_dir = save_dir
        # 创建保存目录
        os.makedirs(self.save_dir, exist_ok=True)
    
    def run(self):
        while True:
            try:
                # 从队列获取图片URL
                img_url = img_queue.get(timeout=5)
                
                try:
                    # 发送请求获取图片内容
                    response = requests.get(img_url, headers=headers, timeout=10)
                    if response.status_code == 200:
                        # 生成文件名
                        file_name = f"image_{downloaded_images.increment()}.jpg"
                        file_path = os.path.join(self.save_dir, file_name)
                        
                        # 保存图片
                        with open(file_path, 'wb') as f:
                            f.write(response.content)
                        
                        print(f"已下载图片: {file_path} 来自 {img_url}")
                    
                    # 随机延时，避免请求过快
                    time.sleep(random.uniform(0.5, 1.5))
                
                except Exception as e:
                    print(f"下载出错: {img_url}, 错误: {str(e)}")
                
                finally:
                    # 标记任务完成
                    img_queue.task_done()
            
            except queue.Empty:
                # 如果HTML解析线程已结束且队列为空，退出循环
                if all(not t.is_alive() for t in html_parsers) and img_queue.empty():
                    break
                # 否则继续等待
                continue

# 主函数
def main():
    start_time = time.time()
    
    # 目标网站
    base_url = "https://example.com"  # 替换为实际网站URL
    
    # 创建并启动URL爬取线程
    url_crawler = UrlCrawler(base_url, max_pages=10)
    url_crawler.start()
    
    # 创建并启动HTML解析线程
    global html_parsers
    html_parsers = [HtmlParser(url_crawler) for _ in range(2)]
    for parser in html_parsers:
        parser.start()
    
    # 创建并启动图片下载线程
    image_downloaders = [ImageDownloader() for _ in range(3)]
    for downloader in image_downloaders:
        downloader.start()
    
    # 等待URL爬取线程结束
    url_crawler.join()
    
    # 等待HTML解析队列处理完毕
    html_queue.join()
    
    # 等待图片下载队列处理完毕
    img_queue.join()
    
    # 等待所有线程结束
    for parser in html_parsers:
        parser.join()
    
    for downloader in image_downloaders:
        downloader.join()
    
    end_time = time.time()
    print(f"爬虫完成，共爬取 {processed_urls.get()} 个页面，下载 {downloaded_images.get()} 张图片")
    print(f"总耗时: {end_time - start_time:.2f} 秒")

if __name__ == "__main__":
    main()
```

## 思考

- **<font color="red">为什么要使用多线程爬虫？</font>** 爬虫是IO密集型任务，使用多线程可以在等待网络响应时执行其他任务，提高效率。

- **<font color="blue">多线程爬虫需要注意哪些问题？</font>** 需要注意线程安全、资源竞争、请求频率控制、异常处理等问题。

- **<font color="green">多线程与多进程爬虫各有什么优缺点？</font>** 多线程共享内存，通信简单但受GIL限制；多进程独立内存空间，可充分利用多核但通信复杂。

- **<font color="purple">如何控制爬虫的爬取速度？</font>** 可以通过设置请求间隔、限制并发线程数、使用延时等方式控制爬取速度。

## 小结

- 进程是操作系统分配资源的基本单位，线程是进程内的执行单元
- Python提供threading模块实现多线程编程
- 多线程间可以通过共享变量、Queue队列等方式进行通信
- GIL限制了Python多线程在CPU密集型任务上的性能提升
- 多线程爬虫适合IO密集型任务，可以显著提高爬取效率

## 总结

本节课我们学习了多任务爬虫的基础知识，深入理解了进程与线程的概念及区别，掌握了Python多线程的创建方法和线程间通信机制。通过实际示例，我们实现了一个完整的多线程爬虫系统，包括URL爬取、HTML解析和图片下载三个主要组件，它们通过队列进行数据传递和任务分配。我们还讨论了GIL对Python多线程的影响，以及如何在实际爬虫开发中合理使用多线程技术。多线程爬虫能够显著提高爬取效率，特别是对于网络IO密集型的爬虫任务，是提升爬虫性能的重要手段。