# 第16节：多任务爬虫2

## 学习目标

- **<font color="red">掌握线程锁的使用方法</font>**
- **<font color="blue">理解主线程与子线程的执行关系</font>**
- **<font color="green">学习多线程方式下载图片</font>**
- **<font color="purple">掌握线程池的创建与使用</font>**
- **<font color="orange">了解多线程爬虫的性能优化</font>**

## 知识点

### 线程锁

- **<font color="red">定义</font>**：控制多个线程对共享资源的访问机制
- **<font color="blue">作用</font>**：防止数据混乱，确保数据一致性
- **<font color="green">类型</font>**：
  - 互斥锁（Lock）：最基本的锁类型
  - 可重入锁（RLock）：同一线程可多次获取
  - 条件锁（Condition）：等待特定条件满足
  - 信号量（Semaphore）：控制并发线程数

### 主线程与子线程

- **<font color="red">主线程</font>**：程序启动时创建的第一个线程
- **<font color="blue">子线程</font>**：由主线程或其他线程创建的线程
- **<font color="green">执行关系</font>**：
  - 默认情况下，主线程结束后程序退出，即使子线程还在运行
  - 设置子线程为守护线程（daemon=True）时，主线程结束子线程自动结束
  - 使用join()方法可让主线程等待子线程执行完毕

### 多线程下载图片

- **<font color="red">实现步骤</font>**：
  - 获取图片URL列表
  - 创建多个下载线程
  - 使用队列分配下载任务
  - 保存下载的图片
- **<font color="blue">优势</font>**：
  - 充分利用网络带宽
  - 显著提高下载效率
  - 适合IO密集型任务

### 线程池

- **<font color="red">定义</font>**：预先创建多个线程，反复利用的线程集合
- **<font color="blue">优势</font>**：
  - 减少线程创建和销毁的开销
  - 控制并发线程数量
  - 提高响应速度
- **<font color="green">实现方式</font>**：
  - concurrent.futures.ThreadPoolExecutor
  - 自定义线程池

## 典型示例

### 线程锁的使用

```python
import threading
import time

# 创建一个锁对象
lock = threading.Lock()

# 共享资源
counter = 0

def increment_with_lock():
    global counter
    for _ in range(100000):
        # 获取锁
        lock.acquire()
        try:
            # 修改共享资源
            counter += 1
        finally:
            # 释放锁
            lock.release()

def increment_without_lock():
    global counter
    for _ in range(100000):
        # 直接修改共享资源（不安全）
        counter += 1

# 使用锁的线程函数
def test_with_lock():
    global counter
    counter = 0
    threads = []
    
    # 创建5个线程
    for _ in range(5):
        t = threading.Thread(target=increment_with_lock)
        threads.append(t)
        t.start()
    
    # 等待所有线程完成
    for t in threads:
        t.join()
    
    print(f"使用锁后的结果: {counter}")

# 不使用锁的线程函数
def test_without_lock():
    global counter
    counter = 0
    threads = []
    
    # 创建5个线程
    for _ in range(5):
        t = threading.Thread(target=increment_without_lock)
        threads.append(t)
        t.start()
    
    # 等待所有线程完成
    for t in threads:
        t.join()
    
    print(f"不使用锁的结果: {counter}")

# 测试
test_with_lock()    # 输出: 使用锁后的结果: 500000
test_without_lock() # 输出: 不使用锁的结果: 小于500000的随机值
```

### 主线程与子线程的关系

```python
import threading
import time

def worker(name, sleep_time):
    print(f"子线程 {name} 开始执行")
    time.sleep(sleep_time)
    print(f"子线程 {name} 执行完毕")

# 创建非守护线程
t1 = threading.Thread(target=worker, args=("t1", 2))
t1.start()

# 创建守护线程
t2 = threading.Thread(target=worker, args=("t2", 5))
t2.daemon = True  # 设置为守护线程
t2.start()

# 使用join等待t1完成
t1.join()
print("主线程执行完毕")

# 此时主线程结束，守护线程t2会被强制终止，不会打印"子线程 t2 执行完毕"
```

### 线程池下载图片

```python
import concurrent.futures
import requests
import os
import time

# 图片URL列表
image_urls = [
    "https://example.com/image1.jpg",
    "https://example.com/image2.jpg",
    "https://example.com/image3.jpg",
    # 更多图片URL...
]

# 下载单个图片的函数
def download_image(url):
    try:
        # 发送请求获取图片内容
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            # 提取文件名
            filename = os.path.basename(url)
            # 保存图片
            with open(f"images/{filename}", "wb") as f:
                f.write(response.content)
            return f"成功下载: {filename}"
        else:
            return f"下载失败: {url}, 状态码: {response.status_code}"
    except Exception as e:
        return f"下载出错: {url}, 错误: {str(e)}"

# 确保保存目录存在
os.makedirs("images", exist_ok=True)

# 使用线程池下载图片
def download_with_threadpool(urls, max_workers=10):
    start_time = time.time()
    
    # 创建线程池
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # 提交下载任务
        future_to_url = {executor.submit(download_image, url): url for url in urls}
        
        # 获取任务结果
        for future in concurrent.futures.as_completed(future_to_url):
            url = future_to_url[future]
            try:
                result = future.result()
                print(result)
            except Exception as e:
                print(f"任务异常: {url}, 错误: {str(e)}")
    
    end_time = time.time()
    print(f"下载完成，总耗时: {end_time - start_time:.2f}秒")

# 执行多线程下载
download_with_threadpool(image_urls)
```

## 实际示例

### 多线程爬取图片并保存

```python
import requests
import threading
import queue
import os
import time
from bs4 import BeautifulSoup
import random

# 设置请求头
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
}

# 创建任务队列和结果队列
url_queue = queue.Queue()
result_queue = queue.Queue()

# 创建锁对象
lock = threading.Lock()

# 已下载计数器
download_count = 0

# 爬取图片URL的函数
def crawl_image_urls(url, max_urls=20):
    try:
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 查找所有图片标签
        img_tags = soup.find_all('img')
        count = 0
        
        for img in img_tags:
            # 获取图片URL
            img_url = img.get('src')
            if img_url:
                # 处理相对路径
                if not img_url.startswith('http'):
                    if img_url.startswith('/'):
                        img_url = f"{url.split('/')[0]}//{url.split('/')[2]}{img_url}"
                    else:
                        img_url = f"{url.rsplit('/', 1)[0]}/{img_url}"
                
                # 将URL放入队列
                url_queue.put(img_url)
                count += 1
                
                if count >= max_urls:
                    break
                    
        print(f"已爬取 {count} 个图片URL")
    except Exception as e:
        print(f"爬取图片URL出错: {str(e)}")

# 下载图片的线程函数
def download_worker():
    global download_count
    
    while True:
        try:
            # 从队列获取URL，设置超时防止无限等待
            url = url_queue.get(timeout=5)
            
            # 如果收到结束信号，则退出
            if url is None:
                break
                
            # 生成文件名
            filename = f"image_{int(time.time())}_{random.randint(1000, 9999)}.jpg"
            filepath = os.path.join("downloaded_images", filename)
            
            # 下载图片
            response = requests.get(url, headers=headers, timeout=10)
            
            # 保存图片
            with open(filepath, 'wb') as f:
                f.write(response.content)
            
            # 更新计数器（使用锁保护）
            with lock:
                download_count += 1
                current_count = download_count
            
            print(f"下载成功: {filename}, 总计: {current_count}")
            
            # 将结果放入结果队列
            result_queue.put((url, filepath, True))
            
            # 随机延时，避免请求过快
            time.sleep(random.uniform(0.1, 0.5))
            
        except queue.Empty:
            # 队列为空，退出循环
            break
        except Exception as e:
            print(f"下载出错: {url if 'url' in locals() else 'Unknown'}, 错误: {str(e)}")
            if 'url' in locals():
                result_queue.put((url, None, False))
        finally:
            # 标记任务完成
            if 'url' in locals() and url is not None:
                url_queue.task_done()

# 主函数
def main():
    # 创建保存目录
    os.makedirs("downloaded_images", exist_ok=True)
    
    # 目标网页URL
    target_url = "https://www.example.com/gallery"  # 替换为实际图片页面URL
    
    # 爬取图片URL
    print("开始爬取图片URL...")
    crawl_image_urls(target_url, max_urls=50)
    
    # 创建下载线程
    thread_count = 5  # 线程数量
    threads = []
    
    print(f"启动 {thread_count} 个下载线程...")
    for i in range(thread_count):
        t = threading.Thread(target=download_worker)
        t.daemon = True  # 设置为守护线程
        threads.append(t)
        t.start()
    
    # 等待队列任务完成
    url_queue.join()
    
    # 发送结束信号给所有线程
    for _ in range(thread_count):
        url_queue.put(None)
    
    # 等待所有线程结束
    for t in threads:
        t.join()
    
    print(f"下载完成，共下载 {download_count} 张图片")

# 执行主函数
if __name__ == "__main__":
    start_time = time.time()
    main()
    end_time = time.time()
    print(f"总耗时: {end_time - start_time:.2f}秒")
```

## 思考

- **<font color="red">为什么需要线程锁？</font>** 多线程环境下，如果不使用锁，可能导致数据竞争和不一致性问题。
- **<font color="blue">线程池与直接创建线程的区别？</font>** 线程池可重用线程，减少创建销毁开销，控制并发数量。
- **<font color="green">如何避免线程死锁？</font>** 合理设计锁的获取顺序，使用超时机制，避免嵌套锁。
- **<font color="purple">多线程爬虫的局限性？</font>** Python的GIL限制了CPU密集型任务的性能提升，更适合IO密集型任务。

## 小结

- 线程锁是多线程编程中保证数据一致性的重要机制
- 理解主线程与子线程的关系有助于正确管理线程生命周期
- 多线程下载可显著提高爬虫效率，特别是对IO密集型任务
- 线程池提供了更高效的线程管理方式
- 合理使用多线程可以在不增加服务器负担的情况下提高爬虫性能

## 总结

本节课我们深入学习了多任务爬虫的进阶知识，重点掌握了线程锁的使用方法、主线程与子线程的执行关系以及多线程下载图片的实现。通过线程锁，我们可以安全地访问共享资源；通过理解线程间的执行关系，我们可以更好地控制程序流程；通过多线程下载技术，我们可以显著提高爬虫的效率。这些技术在实际爬虫开发中具有重要的应用价值，能够帮助我们构建更高效、更稳定的爬虫系统。