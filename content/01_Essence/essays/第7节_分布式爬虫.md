# 第7节：分布式爬虫

## 学习目标

- **<font color="red">理解分布式爬虫的基本概念和应用场景</font>**
- **<font color="blue">掌握分布式爬虫的架构设计和实现方法</font>**
- **<font color="green">学习使用Redis实现任务队列和数据共享</font>**
- **<font color="purple">掌握Scrapy-Redis框架的使用方法</font>**
- **<font color="orange">了解分布式爬虫的性能优化和监控策略</font>**

## 知识点

### 分布式爬虫基础

- **<font color="red">定义</font>**：利用多台机器协同工作的爬虫系统
- **<font color="blue">应用场景</font>**：
  - 大规模数据采集
  - 提高爬取效率
  - 突破单机性能瓶颈
  - 绕过IP访问频率限制
- **<font color="green">优势</font>**：
  - 提高爬取速度
  - 增强系统稳定性
  - 更好的资源利用
  - 更强的容错能力

### 分布式爬虫架构

- **<font color="red">核心组件</font>**：
  - 任务调度器：负责任务的分发和管理
  - 任务执行器：负责执行爬取任务
  - 结果存储器：负责存储爬取结果
  - 监控系统：负责监控爬虫运行状态

- **<font color="blue">架构模式</font>**：
  - 主从模式：一个主节点负责任务分发，多个从节点负责任务执行
  - 对等模式：所有节点地位平等，共同维护任务队列
  - 混合模式：结合主从和对等模式的优点

- **<font color="green">通信方式</font>**：
  - 基于消息队列：如RabbitMQ、Kafka
  - 基于数据库：如Redis、MongoDB
  - 基于RPC：如gRPC、Thrift

### Redis在分布式爬虫中的应用

- **<font color="red">Redis简介</font>**：
  - 高性能的键值对数据库
  - 支持多种数据结构
  - 支持发布/订阅模式
  - 支持主从复制

- **<font color="blue">Redis数据结构</font>**：
  - String：存储URL、网页内容等
  - List：实现任务队列
  - Set：实现URL去重
  - Hash：存储爬虫配置信息
  - Sorted Set：实现优先级队列

- **<font color="green">Redis实现任务队列</font>**：
  ```python
  import redis
  
  # 连接Redis
  r = redis.Redis(host='localhost', port=6379, db=0)
  
  # 生产者：向队列中添加任务
  def add_task(url):
      r.lpush('spider:tasks', url)
  
  # 消费者：从队列中获取任务
  def get_task():
      # BRPOP是阻塞式弹出，如果队列为空，会等待直到有新任务或超时
      task = r.brpop('spider:tasks', timeout=1)
      if task:
          return task[1].decode('utf-8')
      return None
  ```

- **<font color="purple">Redis实现URL去重</font>**：
  ```python
  # 使用Set实现URL去重
  def is_url_visited(url):
      # 如果URL已经存在于集合中，返回0；否则返回1
      return r.sadd('spider:visited_urls', url) == 0
  
  # 标记URL为已访问
  def mark_url_visited(url):
      r.sadd('spider:visited_urls', url)
  ```

### Scrapy-Redis框架

- **<font color="red">Scrapy-Redis简介</font>**：
  - Scrapy的分布式扩展
  - 基于Redis的分布式组件
  - 支持分布式爬取和去重

- **<font color="blue">安装方法</font>**：
  ```bash
  pip install scrapy-redis
  ```

- **<font color="green">核心组件</font>**：
  - RedisDupeFilter：基于Redis的请求去重器
  - RedisScheduler：基于Redis的调度器
  - RedisPipeline：基于Redis的项目管道
  - RedisSpider：基于Redis的爬虫

- **<font color="purple">配置Scrapy-Redis</font>**：
  ```python
  # settings.py
  
  # 启用Redis调度器
  SCHEDULER = "scrapy_redis.scheduler.Scheduler"
  
  # 确保所有爬虫共享相同的去重指纹
  DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
  
  # 将清除的项目在关闭时保存
  SCHEDULER_PERSIST = True
  
  # 配置Redis连接
  REDIS_URL = 'redis://localhost:6379'
  
  # 使用优先级队列（默认）
  SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.PriorityQueue'
  ```

- **<font color="orange">创建RedisSpider</font>**：
  ```python
  from scrapy_redis.spiders import RedisSpider
  
  class MyRedisSpider(RedisSpider):
      name = 'myspider'
      redis_key = 'myspider:start_urls'
  
      def parse(self, response):
          # 处理页面
          pass
  ```

### 分布式爬虫的部署与监控

- **<font color="red">部署方式</font>**：
  - 物理机部署
  - 虚拟机部署
  - Docker容器部署
  - 云服务部署

- **<font color="blue">Docker部署</font>**：
  ```yaml
  # docker-compose.yml
  version: '3'
  services:
    redis:
      image: redis:latest
      ports:
        - "6379:6379"
    
    spider1:
      build: .
      depends_on:
        - redis
      command: scrapy crawl myspider
      environment:
        - REDIS_URL=redis://redis:6379
    
    spider2:
      build: .
      depends_on:
        - redis
      command: scrapy crawl myspider
      environment:
        - REDIS_URL=redis://redis:6379
  ```

- **<font color="green">监控指标</font>**：
  - 爬取速度：每秒请求数、每秒处理的项目数
  - 资源使用：CPU、内存、网络带宽
  - 错误率：请求失败率、解析错误率
  - 队列状态：任务队列长度、处理中的任务数

- **<font color="purple">监控工具</font>**：
  - Prometheus + Grafana：监控系统性能
  - Redis-Stat：监控Redis状态
  - Scrapy内置统计信息
  - 自定义日志和监控脚本

### 分布式爬虫的性能优化

- **<font color="red">网络优化</font>**：
  - 使用异步请求
  - 合理设置并发数
  - 使用代理IP池
  - 优化DNS解析

- **<font color="blue">存储优化</font>**：
  - 批量写入数据
  - 使用异步写入
  - 选择合适的存储介质
  - 定期清理无用数据

- **<font color="green">计算优化</font>**：
  - 减少CPU密集型操作
  - 使用多进程处理
  - 优化解析算法
  - 使用缓存减少重复计算

- **<font color="purple">内存优化</font>**：
  - 控制内存使用
  - 及时释放不需要的资源
  - 避免内存泄漏
  - 使用生成器处理大数据

## 典型示例

### 基于Scrapy-Redis的分布式爬虫

```python
# spiders/redis_spider.py
from scrapy_redis.spiders import RedisSpider
from myproject.items import WebsiteItem

class MyRedisSpider(RedisSpider):
    name = 'redis_spider'
    redis_key = 'redis_spider:start_urls'
    
    def parse(self, response):
        # 提取数据
        item = WebsiteItem()
        item['title'] = response.css('title::text').get()
        item['url'] = response.url
        item['content'] = response.css('body').get()
        yield item
        
        # 提取链接并跟随
        for href in response.css('a::attr(href)').getall():
            yield response.follow(href, self.parse)

# settings.py
# 启用Scrapy-Redis
SCHEDULER = "scrapy_redis.scheduler.Scheduler"
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
SCHEDULER_PERSIST = True
REDIS_URL = 'redis://localhost:6379'

# 启用Redis管道
ITEM_PIPELINES = {
    'scrapy_redis.pipelines.RedisPipeline': 300,
}
```

### 向Redis添加起始URL

```python
# add_start_urls.py
import redis
import json

# 连接Redis
r = redis.Redis(host='localhost', port=6379, db=0)

# 添加起始URL
start_urls = [
    'https://example.com',
    'https://example.org',
    'https://example.net'
]

# 将URL添加到Redis列表中
for url in start_urls:
    r.lpush('redis_spider:start_urls', url)

print(f'Added {len(start_urls)} URLs to redis_spider:start_urls')
```

### 自定义分布式爬虫框架

```python
# distributed_spider.py
import requests
import redis
import threading
import time
from bs4 import BeautifulSoup
import json

class DistributedSpider:
    def __init__(self, redis_host='localhost', redis_port=6379, redis_db=0):
        self.redis_conn = redis.Redis(host=redis_host, port=redis_port, db=redis_db)
        self.task_queue = 'spider:tasks'
        self.result_key = 'spider:results'
        self.visited_key = 'spider:visited'
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
    
    def add_task(self, url):
        """添加爬取任务"""
        if not self.redis_conn.sismember(self.visited_key, url):
            self.redis_conn.lpush(self.task_queue, url)
    
    def get_task(self):
        """获取爬取任务"""
        task = self.redis_conn.brpop(self.task_queue, timeout=1)
        if task:
            return task[1].decode('utf-8')
        return None
    
    def mark_visited(self, url):
        """标记URL为已访问"""
        self.redis_conn.sadd(self.visited_key, url)
    
    def save_result(self, result):
        """保存爬取结果"""
        self.redis_conn.lpush(self.result_key, json.dumps(result))
    
    def process_page(self, url):
        """处理页面，提取数据和链接"""
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # 提取数据
                title = soup.title.text if soup.title else ''
                
                # 保存结果
                result = {
                    'url': url,
                    'title': title,
                    'timestamp': time.time()
                }
                self.save_result(result)
                
                # 提取链接
                for link in soup.find_all('a', href=True):
                    next_url = link['href']
                    if next_url.startswith('http'):
                        self.add_task(next_url)
                
                print(f"Processed: {url}")
            else:
                print(f"Failed to fetch {url}: {response.status_code}")
        except Exception as e:
            print(f"Error processing {url}: {e}")
    
    def worker(self):
        """工作线程"""
        while True:
            url = self.get_task()
            if url:
                if not self.redis_conn.sismember(self.visited_key, url):
                    self.process_page(url)
                    self.mark_visited(url)
            else:
                time.sleep(1)  # 队列为空时等待
    
    def run(self, num_workers=5):
        """启动爬虫"""
        threads = []
        for _ in range(num_workers):
            t = threading.Thread(target=self.worker)
            t.daemon = True
            t.start()
            threads.append(t)
        
        # 等待所有线程完成
        for t in threads:
            t.join()

# 使用示例
if __name__ == "__main__":
    spider = DistributedSpider()
    
    # 添加起始URL
    spider.add_task("https://example.com")
    
    # 启动5个工作线程
    spider.run(num_workers=5)
```

## 实际示例

### 分布式新闻爬虫

```python
# news_spider.py
from scrapy_redis.spiders import RedisSpider
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import Rule
from myproject.items import NewsItem
import datetime

class NewsSpider(RedisSpider):
    name = 'news_spider'
    redis_key = 'news_spider:start_urls'
    
    # 定义链接提取规则
    rules = (
        # 提取新闻页面链接并处理
        Rule(LinkExtractor(allow=r'/article/\d+\.html'), callback='parse_news'),
        # 提取列表页链接并跟随
        Rule(LinkExtractor(allow=r'/list/\d+\.html'), follow=True),
    )
    
    def parse(self, response):
        # 处理首页，提取链接
        for href in response.css('a.news-link::attr(href)').getall():
            yield response.follow(href, self.parse_news)
        
        # 处理分页
        next_page = response.css('a.next-page::attr(href)').get()
        if next_page:
            yield response.follow(next_page, self.parse)
    
    def parse_news(self, response):
        # 提取新闻内容
        news = NewsItem()
        news['title'] = response.css('h1.article-title::text').get()
        news['content'] = ''.join(response.css('div.article-content p::text').getall())
        news['author'] = response.css('span.author-name::text').get()
        news['publish_time'] = response.css('time::attr(datetime)').get()
        news['category'] = response.css('span.category::text').get()
        news['url'] = response.url
        
        yield news

# pipelines.py
import pymongo
from itemadapter import ItemAdapter

class MongoPipeline:
    def __init__(self, mongo_uri, mongo_db):
        self.mongo_uri = mongo_uri
        self.mongo_db = mongo_db
    
    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            mongo_uri=crawler.settings.get('MONGO_URI'),
            mongo_db=crawler.settings.get('MONGO_DATABASE', 'news')
        )
    
    def open_spider(self, spider):
        self.client = pymongo.MongoClient(self.mongo_uri)
        self.db = self.client[self.mongo_db]
    
    def close_spider(self, spider):
        self.client.close()
    
    def process_item(self, item, spider):
        # 使用URL作为唯一标识，实现更新或插入
        self.db[spider.name].update_one(
            {'url': item['url']},
            {'$set': ItemAdapter(item).asdict()},
            upsert=True
        )
        return item

# settings.py
# Redis配置
SCHEDULER = "scrapy_redis.scheduler.Scheduler"
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
SCHEDULER_PERSIST = True
REDIS_URL = 'redis://redis:6379'

# MongoDB配置
MONGO_URI = 'mongodb://mongo:27017/'
MONGO_DATABASE = 'news'

# 启用管道
ITEM_PIPELINES = {
    'myproject.pipelines.MongoPipeline': 300,
}

# docker-compose.yml
version: '3'
services:
  redis:
    image: redis:latest
    ports:
      - "6379:6379"
  
  mongo:
    image: mongo:latest
    ports:
      - "27017:27017"
    volumes:
      - mongo_data:/data/db
  
  spider1:
    build: .
    depends_on:
      - redis
      - mongo
    command: scrapy crawl news_spider
    environment:
      - REDIS_URL=redis://redis:6379
      - MONGO_URI=mongodb://mongo:27017/
  
  spider2:
    build: .
    depends_on:
      - redis
      - mongo
    command: scrapy crawl news_spider
    environment:
      - REDIS_URL=redis://redis:6379
      - MONGO_URI=mongodb://mongo:27017/

volumes:
  mongo_data:
```

### 分布式电商爬虫

```python
# product_spider.py
from scrapy_redis.spiders import RedisSpider
from myproject.items import ProductItem
import json

class ProductSpider(RedisSpider):
    name = 'product_spider'
    redis_key = 'product_spider:start_urls'
    
    def parse(self, response):
        # 处理商品列表页
        for product_link in response.css('div.product-item a.product-link::attr(href)').getall():
            yield response.follow(product_link, self.parse_product)
        
        # 处理分页
        next_page = response.css('a.next-page::attr(href)').get()
        if next_page:
            yield response.follow(next_page, self.parse)
    
    def parse_product(self, response):
        # 提取商品信息
        product = ProductItem()
        product['name'] = response.css('h1.product-name::text').get()
        product['price'] = response.css('span.price::text').get()
        product['description'] = response.css('div.description::text').get()
        product['category'] = response.css('div.breadcrumb span:nth-child(2)::text').get()
        product['url'] = response.url
        
        # 提取商品图片
        product['images'] = response.css('div.product-images img::attr(src)').getall()
        
        # 提取商品规格
        specs = {}
        for spec in response.css('div.specifications tr'):
            key = spec.css('td:nth-child(1)::text').get()
            value = spec.css('td:nth-child(2)::text').get()
            if key and value:
                specs[key.strip()] = value.strip()
        product['specifications'] = specs
        
        yield product

# middlewares.py
import random
from scrapy.downloadermiddlewares.useragent import UserAgentMiddleware
from scrapy.downloadermiddlewares.retry import RetryMiddleware
from scrapy.utils.response import response_status_message
import time

class RandomUserAgentMiddleware(UserAgentMiddleware):
    def __init__(self, user_agent_list):
        self.user_agent_list = user_agent_list
    
    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            user_agent_list=crawler.settings.getlist('USER_AGENT_LIST')
        )
    
    def process_request(self, request, spider):
        request.headers['User-Agent'] = random.choice(self.user_agent_list)

class CustomRetryMiddleware(RetryMiddleware):
    def process_response(self, request, response, spider):
        if response.status in self.retry_http_codes:
            reason = response_status_message(response.status)
            # 添加随机延迟，避免频繁重试
            time.sleep(random.uniform(1, 3))
            return self._retry(request, reason, spider) or response
        return response

# settings.py
# 用户代理列表
USER_AGENT_LIST = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0'
]

# 启用中间件
DOWNLOADER_MIDDLEWARES = {
    'myproject.middlewares.RandomUserAgentMiddleware': 400,
    'myproject.middlewares.CustomRetryMiddleware': 500,
}

# 重试设置
RETRY_ENABLED = True
RETRY_TIMES = 3
RETRY_HTTP_CODES = [500, 502, 503, 504, 408, 429]

# 下载延迟
DOWNLOAD_DELAY = 1
RANDOMIZE_DOWNLOAD_DELAY = True
```

## 思考题

1. 分布式爬虫相比单机爬虫有哪些优势和挑战？如何权衡使用分布式爬虫的成本和收益？
2. Redis在分布式爬虫中扮演什么角色？除了Redis，还有哪些技术可以用于实现分布式爬虫的任务调度和数据共享？
3. 如何设计一个可扩展的分布式爬虫系统，使其能够根据任务量自动调整爬虫节点数量？
4. 分布式爬虫中如何处理反爬虫策略？特别是当多个爬虫节点共享IP资源时，如何避免被目标网站封禁？
5. 如何监控和调试分布式爬虫系统？当系统出现问题时，如何快速定位和解决？

## 小结

- **<font color="red">分布式爬虫通过多机协作，显著提高了数据采集的效率和规模</font>**
- **<font color="blue">Redis作为核心组件，为分布式爬虫提供了任务队列、URL去重和数据共享功能</font>**
- **<font color="green">Scrapy-Redis框架简化了分布式爬虫的开发，提供了现成的分布式组件</font>**
- **<font color="purple">合理的架构设计和优化策略是构建高效分布式爬虫的关键</font>**
- **<font color="orange">监控和部署工具帮助维护分布式爬虫的稳定运行</font>**

## 总结

分布式爬虫是解决大规模数据采集问题的有效方案。通过将爬取任务分散到多台机器上并行执行，分布式爬虫能够突破单机性能瓶颈，提高爬取效率，增强系统稳定性，并有效规避IP访问频率限制。

本节课介绍了分布式爬虫的基本概念、架构设计和实现方法。我们学习了如何使用Redis作为核心组件，实现任务队列、URL去重和数据共享功能。Scrapy-Redis框架为我们提供了现成的分布式爬虫解决方案，简化了开发过程。

在实际应用中，分布式爬虫需要考虑网络优化、存储优化、计算优化和内存优化等多个方面，以提高系统性能。同时，合理的部署策略和监控工具对于维护分布式爬虫的稳定运行至关重要。

通过掌握分布式爬虫技术，我们能够构建更加高效、稳定和可扩展的数据采集系统，为大数据分析和人工智能应用提供坚实的数据基础。