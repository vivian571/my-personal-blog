# 第8节：实战案例

## 学习目标

- **<font color="red">掌握爬取电影网站数据的完整流程</font>**
- **<font color="blue">学习数据清洗与存储的实用技巧</font>**
- **<font color="green">理解爬虫项目的架构设计</font>**
- **<font color="purple">掌握数据可视化的基本方法</font>**
- **<font color="orange">学习应对常见反爬措施的策略</font>**

## 知识点

### 项目规划

- **<font color="red">需求分析</font>**：明确爬取目标、数据用途和技术选型
- **<font color="blue">架构设计</font>**：
  - 数据采集模块
  - 数据处理模块
  - 数据存储模块
  - 数据展示模块
- **<font color="green">技术选型</font>**：
  - 请求库：requests
  - 解析库：BeautifulSoup/正则表达式
  - 存储：CSV/JSON/数据库

### 数据采集

- **<font color="red">目标网站分析</font>**：
  - 网站结构
  - 数据位置
  - 反爬机制
- **<font color="blue">请求构造</font>**：
  - Headers设置
  - 代理IP使用
  - 请求频率控制
- **<font color="green">数据提取</font>**：
  - 电影名称
  - 评分
  - 引言
  - 详情页URL

### 数据处理

- **<font color="red">数据清洗</font>**：
  - 去除无用字符
  - 格式统一化
  - 缺失值处理
- **<font color="blue">数据结构化</font>**：
  - 字典/列表组织
  - JSON格式转换
  - 数据表设计

### 数据存储

- **<font color="red">文件存储</font>**：
  - CSV格式
  - JSON格式
- **<font color="blue">数据库存储</font>**：
  - SQLite简单应用
  - MySQL基础操作

## 典型示例

### 豆瓣电影Top10爬取

```python
import requests
from bs4 import BeautifulSoup
import csv
import time
import random

# 设置请求头
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
}

# 爬取豆瓣电影Top250的前10部电影
def crawl_douban_top10():
    base_url = 'https://movie.douban.com/top250'
    movies = []
    
    # 只爬取第一页的前10部电影
    response = requests.get(base_url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # 提取电影信息
    movie_items = soup.select('.grid_view li')
    for item in movie_items[:10]:  # 只取前10部
        title = item.select_one('.title').text
        rating = item.select_one('.rating_num').text
        quote = item.select_one('.quote .inq')
        quote = quote.text if quote else ''
        info = item.select_one('.bd p').text.strip()
        
        # 提取导演和主演信息
        info_parts = info.split('\n')
        director_actors = info_parts[0].strip()
        
        # 提取年份、国家和类型
        year_country_type = info_parts[1].strip() if len(info_parts) > 1 else ''
        
        # 存储电影信息
        movie = {
            'title': title,
            'rating': rating,
            'quote': quote,
            'director_actors': director_actors,
            'year_country_type': year_country_type
        }
        movies.append(movie)
        
        # 随机延时，避免被反爬
        time.sleep(random.uniform(0.5, 2))
    
    return movies

# 保存为CSV文件
def save_to_csv(movies, filename='douban_top10.csv'):
    with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=movies[0].keys())
        writer.writeheader()
        writer.writerows(movies)
    print(f'数据已保存到{filename}')

# 主函数
def main():
    print('开始爬取豆瓣电影Top10...')
    movies = crawl_douban_top10()
    save_to_csv(movies)
    print(f'成功爬取{len(movies)}部电影信息')

if __name__ == '__main__':
    main()
```

## 实际示例

### 完整的豆瓣电影爬虫项目

```python
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import sqlite3
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

# 设置中文字体，解决matplotlib中文显示问题
plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号

class DoubanMovieCrawler:
    def __init__(self):
        self.base_url = 'https://movie.douban.com/top250'
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
        }
        self.movies = []
        self.db_name = 'douban_movies.db'
    
    def get_page(self, page=0):
        """获取指定页面的HTML内容"""
        params = {'start': page * 25, 'filter': ''}
        try:
            response = requests.get(self.base_url, headers=self.headers, params=params)
            if response.status_code == 200:
                return response.text
            else:
                print(f"请求失败: {response.status_code}")
                return None
        except Exception as e:
            print(f"请求异常: {e}")
            return None
    
    def parse_page(self, html):
        """解析页面，提取电影信息"""
        soup = BeautifulSoup(html, 'html.parser')
        movie_items = soup.select('.grid_view li')
        
        page_movies = []
        for item in movie_items:
            # 提取基本信息
            rank = item.select_one('.pic em').text
            title = item.select_one('.title').text
            rating = item.select_one('.rating_num').text
            vote_count = item.select_one('.star span:last-child').text.strip('(').strip('人评价)')
            
            # 提取引言（可能不存在）
            quote = item.select_one('.quote .inq')
            quote = quote.text if quote else ''
            
            # 提取详细信息
            info = item.select_one('.bd p').text.strip()
            info_parts = info.split('\n')
            director_actors = info_parts[0].strip()
            
            # 提取年份、国家和类型
            year_country_type = info_parts[1].strip() if len(info_parts) > 1 else ''
            
            # 尝试提取年份
            year = ''
            if year_country_type:
                try:
                    year = year_country_type.split('/')[0].strip()
                    # 确保年份是4位数字
                    year = ''.join(filter(str.isdigit, year))[:4]
                except:
                    pass
            
            # 存储电影信息
            movie = {
                'rank': int(rank),
                'title': title,
                'rating': float(rating),
                'vote_count': int(vote_count.replace(',', '')),
                'quote': quote,
                'director_actors': director_actors,
                'year_country_type': year_country_type,
                'year': year
            }
            page_movies.append(movie)
        
        return page_movies
    
    def crawl(self, max_pages=10):
        """爬取指定页数的电影信息"""
        for page in tqdm(range(max_pages), desc="爬取进度"):
            html = self.get_page(page)
            if html:
                page_movies = self.parse_page(html)
                self.movies.extend(page_movies)
                # 随机延时，避免被反爬
                time.sleep(random.uniform(1, 3))
            else:
                break
        
        print(f"成功爬取{len(self.movies)}部电影信息")
        return self.movies
    
    def save_to_csv(self, filename='douban_movies.csv'):
        """保存数据到CSV文件"""
        if not self.movies:
            print("没有数据可保存")
            return
        
        df = pd.DataFrame(self.movies)
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"数据已保存到{filename}")
    
    def save_to_database(self):
        """保存数据到SQLite数据库"""
        if not self.movies:
            print("没有数据可保存")
            return
        
        conn = sqlite3.connect(self.db_name)
        cursor = conn.cursor()
        
        # 创建表
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS movies (
            rank INTEGER PRIMARY KEY,
            title TEXT,
            rating REAL,
            vote_count INTEGER,
            quote TEXT,
            director_actors TEXT,
            year_country_type TEXT,
            year TEXT
        )
        ''')
        
        # 插入数据
        for movie in self.movies:
            cursor.execute('''
            INSERT OR REPLACE INTO movies (rank, title, rating, vote_count, quote, director_actors, year_country_type, year)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                movie['rank'],
                movie['title'],
                movie['rating'],
                movie['vote_count'],
                movie['quote'],
                movie['director_actors'],
                movie['year_country_type'],
                movie['year']
            ))
        
        conn.commit()
        conn.close()
        print(f"数据已保存到{self.db_name}")
    
    def data_analysis(self):
        """对爬取的数据进行简单分析"""
        if not self.movies:
            print("没有数据可分析")
            return
        
        df = pd.DataFrame(self.movies)
        
        # 1. 评分分布
        plt.figure(figsize=(10, 6))
        sns.histplot(df['rating'], bins=20, kde=True)
        plt.title('豆瓣Top250电影评分分布')
        plt.xlabel('评分')
        plt.ylabel('电影数量')
        plt.savefig('rating_distribution.png')
        plt.close()
        
        # 2. 年代分布
        # 确保year列是数字类型
        df['year'] = pd.to_numeric(df['year'], errors='coerce')
        df['decade'] = (df['year'] // 10) * 10  # 按十年分组
        
        decade_counts = df['decade'].value_counts().sort_index()
        plt.figure(figsize=(12, 6))
        decade_counts.plot(kind='bar')
        plt.title('豆瓣Top250电影年代分布')
        plt.xlabel('年代')
        plt.ylabel('电影数量')
        plt.xticks(rotation=45)
        plt.savefig('decade_distribution.png')
        plt.close()
        
        # 3. 评分与投票数关系
        plt.figure(figsize=(10, 6))
        sns.scatterplot(x='vote_count', y='rating', data=df)
        plt.title('评分与投票数关系')
        plt.xlabel('投票数')
        plt.ylabel('评分')
        plt.xscale('log')  # 使用对数刻度，因为投票数差异很大
        plt.savefig('rating_vs_votes.png')
        plt.close()
        
        print("数据分析完成，图表已保存")

# 主函数
def main():
    crawler = DoubanMovieCrawler()
    crawler.crawl(max_pages=1)  # 只爬取第一页作为示例
    crawler.save_to_csv()
    crawler.save_to_database()
    crawler.data_analysis()

if __name__ == '__main__':
    main()
```

## 思考题

1. 如何优化爬虫的效率，在不被反爬的情况下提高爬取速度？
2. 对于大规模数据爬取，如何设计更合理的存储方案？
3. 如何处理爬取过程中可能出现的异常情况，如网络中断、服务器拒绝等？
4. 如何设计一个更灵活的爬虫框架，使其能够适应不同网站的爬取需求？
5. 爬取数据后，如何进行更深入的数据分析和可视化？

## 小结

- **<font color="red">实战案例是将爬虫理论知识应用到实际问题中的重要环节</font>**
- **<font color="blue">完整的爬虫项目包括数据采集、处理、存储和分析等多个环节</font>**
- **<font color="green">合理的架构设计和模块划分是构建可维护爬虫项目的关键</font>**
- **<font color="purple">数据清洗和结构化处理对于后续的数据分析至关重要</font>**
- **<font color="orange">在实际爬取过程中，需要综合运用多种技术手段应对各种挑战</font>**

## 总结

本节课通过豆瓣电影Top250的爬取案例，展示了一个完整爬虫项目的开发流程。从项目规划、架构设计到具体实现，我们详细讲解了数据采集、处理、存储和分析的各个环节。通过这个实战案例，学习者可以将前面学习的各种爬虫技术综合应用，掌握构建完整爬虫项目的方法。在实际开发中，还需要根据具体需求和目标网站的特点，灵活运用各种技术，不断优化爬虫的性能和稳定性。