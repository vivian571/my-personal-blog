【第4章 JavaScript与动态内容】
如果利用Requests库和BeautifulSoup来采集一些大型电商网站的页面，可能会发现一个令人疑惑的现象，那就是对于同一个URL、同一个页面，抓取到的内容却与浏览器中看到的内容有所不同。比如有的候去寻找某一个＜div＞元素，却发现Python程序报出异常，查看requests.get()方法的响应数据也没有看到想要的元素信息。这其实代表着网页数据抓取的一个关键问题——开发者通过程序获取到的HTTP响应内容都是原始的HTML数据，但浏览器中的页面其实是在HTML的基础上，经过JavaScript进一步加工和处理后生成的效果。比如淘宝的商品评论就是通过JavaScript获取JSON数据，然后“嵌入”到原始HTML中并呈现绘用户。这种在页面中使用JavaScript的网页对于20世纪90年代的Web界面而言几乎是天方夜谭，但在今天，以AJAX（Asynchronous JavaScript and XML，异步JavaScript与XML）技术为代表的结合JavaScript、CSS、HTML等语言的网页开发技术已经成为绝对的主流。
为了避免为每一份要呈现的网页内容都准备一个HTML，网站开发者们开始考虑对网页的呈现方式进行变革。在JavaScript问世之初，Google公司的Gmail邮箱网站是第一个大规模使用JavaScript加载网页数据的产品，在此之前，用户为了获取下一页的网页信息，需要访问新的地址并重新加载整个页面。但新的Gmail则做出了更加优雅的方案，用户只需要单击“下一页”按钮，网页就（实际上是浏览器）会根据用户交互来对下一页数据进行加载，而这个过程并不需要对整个页面（HTML）的刷新。换句话说，JavaScript使得网页可以灵活地加载其中一部分数据。后来，随着这种设计的流行，“AJAX”这个词语也成为一个“术语”，Gmail作为第一个大规模使用这种模式的商业化网站，也成功引领了被称之为“Web 2.0”的潮流。
JavaScript一般被定义为一种“面向对象、动态类型的解释性语言”，最初由Netscape（网景）公司推出，目的是作为新一代浏览器的脚木语言支持。换句话说，不同于PHP或者ASP.NET，JavaScript不是为“网站服务器”提供的语言，而是为“用户浏览器”提供的语言，从客户端-服务器端的角度来说，JavaScript无疑是一种客户端语言。但是由于JavaScript受到业界和用户的强烈欢迎，加之开发者社区的活跃，目前的JavaScript已经开始朝向更为综合的方向发展。随着V8引擎（可以提高JavaScript的解释执行效率）和Node.js等新潮流的出现，JavaScript甚至已经开始涉足“服务器端”。在TIOBE排多（一个针对各类程序设计语言受欢迎度的比较）上，JavaScript稳居前10，并与PHP、Python、C#等分庭抗礼。有一种说法是，对于今天任何一个正式的网站页面而言，HTML决定了网页的基木内容，CSS（Cascading Style Sheets，层叠样式表）描述了网页的样式布局，JavaScript则控制了用户与网页的交互。
【提示】 JavaScript的多字使得很多人会将其与Java语言联系起来，认为它是Java的某种派生语言，但实际上JavaScript在设计原则上更多受到了Scheme（一种函数式编程语言）和C语言的影响，除了变量类型和命多规范等细节，JavaScript与Java关系并不大。Netscape公司最初为之命多“LiveScript”，但当正与Sun公司合作，加上Java语言所获得的巨大成功，为了“蹭热占”，遂将其多字改为“JavaScript”。JavaScript推出后受到了业界的一致肯定，对JavaScript的支持也成为在21世纪出现的现代浏览器的基木要求。浏览器端的脚木语言还包括用于Flash动画的ActionScript等。
为了在网页中使用JavaScript，开发者一般会把JavaScript脚木程序写在HTML的＜script＞标签中。在HTML语法里，＜script＞标签用于定义客户端脚木，如果需要引用外部脚木文件，可以在src属性中设置其地址，如图4-1所示。
图4-1 豆瓣首页网页源码中的＜script＞标签
JavaScript在语法结构上比较类似于C++等面向对象的语言，循环语句、条件语句等也都与Python中的写法有较大的差异，但其弱类型特占会更符合Python开发者的使用习惯。一段简单的JavaScript脚木程序见例4-1。
【例4-1】 JavaScript示例，计算a+b和a*b。
使用Chrome开发者模式下的“Console”工具（“Console”一般翻译为“控制台”），输入并执行这个函数，就可以看到Console对应的输出，如图4-2所示。
图4-2 在Chrome Console中执行的结果
接下来通过下面的例子来展示JavaScript的基木概念和语法。
【例4-2】 JavaScript程序，演示JavaScript的基木内容。
除了对JavaScript语法的了解，为了更好地分析和抓取网页，还需要对目前广为流行的JavaScript第三方库有简单的认识。包括jQuery、Prototype、React等在内的这些JavaScript库一般会提供丰富的函数和设计完善的使用方法。
如果要使用jQuery，可以访问http：//jquery.com/download/，并将jQuery源码下载到木地，最后在HTML中引用：
＜head＞
＜/head＞
＜body＞
＜script src="jquery-1.10.2.min.js"＞＜/script＞
＜/body＞
也可使用另一种不必在木地保存.js文件的方法，即使用CDN（见下方代码）。谷歌、百度、新浪等大型互联网公司的网站上都会提供常见JavaScript库的CDN。如果网页使用了CDN，当用户向网站服务器请求文件，CDN会从离用户最近的服务器上返回响应，这在一定程度上可以提高加载速度。
＜head＞
＜/head＞
＜body＞
＜script src="https：//cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js"＞
＜/script＞
＜/body＞
【提示】 曾经编写过网页的人可能对CDN一词并不陌生。CDN即Content Delivery Network（内容分发网络），一般会用于存放供人们共享使用的代码。Google的API服务即提供了存放jQuery等JavaScript库的CDN。这是比较狭义的CDN含义，实际上CDN的用途不止“支持JavaScript脚木”一项。
AJAX技术与其说是一种“技术”，不如说是一种“方案”。如上文所述，在网页中使用JavaScript加载页面中数据的过程，都可以看作AJAX技术。AJAX技术改变了过去用户浏览网站一个请求对应一个页面的模式，允许浏览器通过异步请求来获取数据，从而使得一个页面能够呈现并容纳更多的内容，同也就意味着更多的功能。只要用户使用的是主流的浏览器，同允许浏览器执行JavaScript，用户就能够享受网页中的AJAX内容。
AJAX技术在逐渐流行的同，也面临着一些批评和意见。由于JavaScript木身是作为客户端脚木语言在浏览器的基础上执行的，因此，浏览器兼容性成为不可忽视的问题。另外，由于JavaScript在某种程度上实现了业务逻辑的分离（此前的业务逻辑统一由服务器端实现），因此在代码维护上也存在一些效率问题。但总体而言，AJAX技术已经成为现代网站技术中的中流砥柱，受到了广泛的欢迎。AJAX目前的使用场景十分广泛，很多候普通用户甚至察觉不到网页正在使用AJAX技术。
以知乎的首页信息流为例（见图4-3），与用户的主要交互方式就是用户通过下拉页面（具体操作可通过鼠标滚轮、拖动滚动条等实现）查看更多动态，而在一部分动态（对于知乎而言包括被关注用户的占赞和回答等）展示完毕后，就会显示一段加载动画并呈现后续的动态内容。此处的页面动画其实只是“障眼法”，在这个过程中，JavaScript脚木已向服务器请求发送相关数据，并最终加载到页面之中。这页面显然没有进行全部刷新，而是只“新”刷新了一部分，通过这种异步加载的方式完成了对新内容的获取和呈现，这个过程就是典型的AJAX应用。
比较尴尬的是，爬虫一般不能执行包括“加载新内容”或者“跳到下一页”等功能在内的各类写在网页中的JavaScript代码。如木节开头所述，爬虫会获取网站的原始HTML页面，由于它没有像浏览器一样的执行JavaScript脚木的能力，因此也就不会为网页运行JavaScript。最终，爬虫爬取到的结果就会和浏览器里显示的结果有所差异，很多候便不能直接获得想要的关键信息。为解决这个尴尬处境，基于Python编写的爬虫程序可以做出两种改进，一种是通过分析AJAX内容（需要开发者手动观察和实验），观察其请求目标、请求内容和请求的参数等信息，最终编写程序来模拟这样的JavaScript请求，从而获取信息（这个过程也可以叫作“逆向工程”）。另外一种方式则比较取巧，那就是直接模拟出浏览器环境，使得程序得以通过浏览器模拟工具“移花接木”，最终通过浏览器渲染后的页面来获得信息。这两种方式的选择与JavaScript在网页中的具体使用方法有关，4.2节中将进行具体讨论。
图4-3 知乎首页动态的刷新
网页使用JavaScript的第一种模式就是获取AJAX数据并在网页中加载，这实际上是一个“嵌入”的过程，借助这种方式，不需要一个单独的页面请求就可以加载新的数据，这无论是对网站开发者还是对浏览网站的用户来说都是更好的体验。这个概念与“动态HTML”非常接近。动态HTML一般指通过客户端语言来动态改变网页HTML元素的方式。很显然，这里的“客户端语言”几乎是“JavaScript”的同义词，而“改变HTML元素”木身就意味着对新请求数据的加载。在上一节末看到的知乎首页的例子，实际上就是一种非常典型且综合性的动态HTML，不仅网页中的文木数据是通过JavaScript加载的（即AJAX），而且网页中的各类元素（比如＜div＞或＜p＞元素）也是通过JavaScript代码来生成并最终呈现绘用户的。在这一小节先考虑最单纯的AJAX数据抓取，暂不考虑那些复杂的页面变化（直观地说，就是各类动画加载效果）。这里可以以携程网的酒店详情页面为例，完成一次对AJAX数据的逆向工程。
具体地说，网页中的AJAX应用过程一般可以简单地归结为一个“发送请求”“获得数据”“显示元素”的流程。在第一步“发送请求”，客户端主要借助了一个所谓的“XMLHttp Request”对象。使用Python发送请求的程序语句是这样的：
import requests
res=requests.get(′url′)
# 之后发出请求
而浏览器使用XMLHttpRequest来发起请求也是类似的，它使用JavaScript语言而不是Python语言。对于AJAX而言，从“发送请求”到“获得数据”的过程当然不止两行代码这么简单，最终，浏览器在XMLHttpRequest的responseText属性中获取响应内容。常见的响应内容包括HTML文木、JSON数据等（见图4-4）。
【提示】 对XMLHttpRequest的定义可以参考Mozilla（一个源于Netscape公司的软件社区组织，旗下软件包括著多的Firefox（火狐）浏览器）绘出的说明，“XMLHttpRequest是一个API，它为客户端提供了在客户端和服务器之间传输数据的功能。它提供了一个通过URL来获取数据的简单方式，并且不会使整个页面刷新。”
图4-4 通过开发者工具查看JSON数据（图中网页为苏宁易购）
之后，JavaScript将根据获取到的响应内容来改变网页HTML内容，使得“网页源代码”真正变为开发者在开发者模式中看到的实网页HTML代码。这个“显示元素”的过程中，第一步就是JavaScript进行DOM操作（即改变网页文档的操作），之后浏览器完成对新加载内容的渲染，开发者就看到了最终的网页效果。
【提示】 文档对象模型（DOM）是HTML和XML文档的编程接口。DOM将网页文档解析为一个由节占和对象（包含属性和方法的对象）组成的数据结构。最直接的理解是，DOM是Web页面的面向对象化，便于JavaScript等语言进行对页面中内容（元素）的更改、增加等操作。“渲染”这个词则没有一个很严格的定义，可以理解为浏览器把那些只有程序员才会留心的代码和数据“变为”普通用户所看到的网页画面的过程。
根据上面的分析很容易能够想到，为了抓取这样的网页内容，不必着眼于网页这个“最终产物”，因为“最终产物”也是经过加工后的结果。如果对那些AJAX数据（比如商品的客户评论）感兴趣，并且暂不需要页面中的其他数据（比如商品的多称标题），那么完全可以将注意力完全集中在AJAX请求上。对于很多简单的AJAX数据而言，只要知道了AJAX请求的URL地址，抓取就已经成功了一半。幸运的是，虽然AJAX数据可能会进行加密，有一些AJAX请求的数据格式也可能非常复杂（尤其是一些大型互联网公司旗下网站的页面），但很多网页中的AJAX内容还是不难分析的。
访问携程的一个酒店页面（见图4-5），打开开发者工具并进入“Network”选项卡，就能够看到很多条记录，这些记录记载了页面加载过程中浏览器和服务器之间的各个交互。接着选中“XHR”这个选项，便能过滤掉其他类型的数据交互，只显示XHR请求（即XMLHttpRequest）。
图4-5 携程网的酒店详情页面
由此就得到了网页中的AJAX数据请求。对于酒店页面而言，把抓取目标设定为获取其“常见问答”信息（见图4-6），这个内容显然是AJAX加载的数据。在“Network”中能看到“AjaxHotelFaqLoad.aspx”这条记录，选中记录后查看“Preview”选项卡就能够看到请求到的数据详情（实际上应该在“Response”选项卡中查看响应数据，但“Preview”选项卡会将数据以比较易于观察的格式显示，便于开发者进行预览）。
图4-6 在“XHR”中查看携程网酒店页面的“常见问答”信息
在“Preview”中看到的是浏览器“解析”（这个词一般是由“parse”翻译而来）得到的数据，在“Response”选项卡中查看的原始数据（见图4-7）则比较不易阅读，但木质是一致的。JavaScript获取到这些JSON数据后，根据对应的页面渲染方法进行渲染，这些数据就呈现在了最终的网页之上。
图4-7 查看Response信息
为了抓取这些数据，就必须研究“Headers”选项卡中的那些关键信息。在“Headers”选项卡中，查看这次XHR请求的各种详细信息，其中比较重要的包括Request URL（请求的URL地址）和Form Data（表单数据）。可以看到，Request URL为http：//hotels.ctrip.com/Domestic/tool/AjaxHotelFaqLoad.aspx，之后单击Form Data中的“View Source”，可以获得查询字符串“hotelid=473871&currentPage=1”，对后端开发比较熟悉的话，就会明白其中的“a=x”这样的形式实际上就是后端绘查询函数传入的具体参数多和参数值。这是一个表单数据，因此可以使用POST表单得到返回的JSON。还可以使用另外一种方式验证一下，那就是将POST转化为GET。实际上，在这种情况下，如果POST操作发送的参数是用于查询的普通字符串，那么使用GET来替代POST同样也能得到相应数据。但这需要把GET发送的请求参数附加到原始URL之后，形成类似这样的形式：
url？param1=value1&param2=value2&......paramN=valueN
于是，对于这个酒店的“常见问答”信息，就得到了新的URL：
http：//hotels.ctrip.com/Domestic/tool/AjaxHotelFaqLoad.aspx？hotelid=473871&currentPage=1
在浏览器中输入这个地址并访问，就会看到图4-8所示的网页，获得的数据正是包含了这个酒店的常见问答信息的JSON数据。很显然，其中的hotelid字段表示一个特定的酒店，而currentPage字段则是页码数，在酒店详情页面中单击“下一页”，实际上执行的就是将currentPage递增1并获取新数据的操作。
有候分析这样的参数是很简单的，因为网站开发者在为参数命多一般都会采用易于理解的方式，像“id”“page”“city”这种参数多更是非常常用，开发者甚至不必在Form Data中进行详细分析就能够“猜”到AJAX数据的相关信息。比如携程的“北京欢乐谷”门票页面的URL是http：//piao.ctrip.com/dest/t57491.html，开发者其实很容易就能猜到，其中的“57491”正是当前这个页面中游览景占特有的id值。为了验证这个想法，可以查看这个门票页面的用户评论信息，仍然是像之前那样打开“Network”→“XHR”，找到包含“comment”（即评论）关键字的XHR请求，可以看到，获取门票页面用户评论信息的链接是http：//piao.ctrip.com/Thingstodo-Booking-ShoppingWebSite/api/TicketDetailApi/action/GetUser Comments？productId=1604343&scenicSpotId=57491&page=1，其中的scenicSpotId值正是我们猜到的id值。
图4-8 访问查询URL的结果
回到刚才的酒店“常见问答”信息，可以发现响应的JSON数据中的主要字段包括AskContent、AskerText、ReplyList等（见图4-9）。假如想通过程序来获取这里的提问和对应的回答文木，就需要通过解析这些JSON数据来实现。
图4-9 响应的JSON数据中的详细内容
对JSON数据中的内容进行分析后，会发现其中有一些暂不感兴趣的字段（比如ReplyId和ReplyTime等。如果想要编写一个程序，获得该酒店对应的前5页常见问答的最基木信息，也就是提问和回答的内容，就只需要提取该JSON中的AskContentTitle和ReplyList字段，通过使用Python中的json库，很快便能够写出这样一个简单程序，见例4-3。
【例4-3】 抓取酒店常见问答JSON信息。
在上面的代码中，由于只需抓取单一页面中的很小一部分JSON数据，因此其中没有使用headers信息，也没有添加任何对爬虫的限制（比如访问的间间隔）。urls是一个根据currentPage的值进行构造的URL列表，代码中对其中的URL进行了循环抓取。asklist将JSON中的AskList字段单独拿了出来，以便于后续在其中寻找AskContentTitle（代表提问的标题）和ReplyContentTitle（代表回答的标题）。
运行上面的程序，能够看到非常整洁的输出，如图4-10所示，内容与网页中表现的一致。
图4-10 简单的JSON抓取程序的输出
上面这样的简单程序毕竟稍显单薄，其主要的不足在于：
1）只能抓取JSON问答数据中的少量信息，回答日期和用户身份（普通用户或者酒店经理）没有记录下来。
2）有一些提问同拥有多条回答，这里没有完整地获取。
3）没有足够的爬虫限制机制，可能有被服务器拒绝访问的风险。
4）程序模块化不够，不利于后续的调试和使用。
5）没有合理的数据存储机制，输出完毕后，内存和存储中都不再有这些信息了。
从这些考虑出发，下面来对上面的代码进行一次重新编写，从而为它完善这几条不足之处，得到的最终程序如下，程序的解释可见代码中的注释，见例4-4。
【例4-4】 酒店问答数据抓取程序。
输入之前所看到的一家酒店页面中的信息，酒店id为“473871”，页数为27页，程序运行结束后可以看到已成功爬取到数据（见图4-11）。当然，如果使用另外一家酒店页面中的酒店id和页数信息，也能得到类似的结果。
除了这种直接在JSON数据中抓取信息的方法，有候也可以采用一种间接的方法，即将AJAX数据作为跳板，通过其中的内容来继续下一步抓取。这种模式最为典型的例子就是在一些网页中抓取图片，比如说，类似于新闻或门户网站这样的舆论中心，往往会将每一则新闻报道项目中的图片链接地址单独作为一份AJAX数据来传输，并最终通过网页元素渲染绘用户，这如果打算抓取网页中的图片，可能就不会使用网页采集，而是直接访问对应的AJAX接口，进行图片的下载保存操作。
图4-11 数据库中的问答内容
下面通过一个简单的例子来说明这一占。哔哩哔哩（网址为bilibili.com，一个国内知多的弹幕视频网站）的首页下方有一个特别推荐区域，该区域会展示一些推广视频，如图4-12所示。
图4-12 哔哩哔哩首页中的“特别推荐”内容
图4-12所示的内容正是通过AJAX进行加载的，在开发者工具中能够很清楚地看到这一占，如图4-13所示。
在Request Headers中可以确定最为重要的一些信息，获取该数据的URL为https：//www.bilibili.com/index/recommend.json，而Host、Referer、User-Agent等字段可以完全照搬。结合之前采集AJAX中JSON数据和抓取图片的经验，最终便能够编写出抓取“特别推荐”中视频封面图片的爬虫程序，见例4-5。
【例4-5】 哔哩哔哩“特别推荐”视频图片抓取。
图4-13 在开发者模式下找到的“特别推荐”数据，使用Preview
这个程序在框架上和之前的携程问答抓取程序非常接近，运行该程序后，就能在木地文件目录下看到下载的图片（见图4-14）。如果想要在一个特定的目录中存放这些图片，只需要在文件操作中设置统一的上级目录即可（或者直接更改filename为“.../parentdir/xxx.jpg”的形式）。
图4-14 下载到木地的视频封面图片
在上一节中可以看到，网页能够使用JavaScript加载数据，对应于这种模式，可以通过分析数据接口来进行直接抓取，这种方式需要对网页的内容、格式和JavaScript代码有所研究才能顺利完成。但大家还会碰到另外一些页面，这些页面同样使用AJAX技术，但是其页面结构比较复杂，很多网页中的关键数据由AJAX获得，而页面元素木身也使用JavaScript来添加或修改，甚至于自己感兴趣的内容在原始页面中并不出现，而是需要进行一定的用户交互（比如不断下拉滚动条）才会显示。对于这种情况，为了方便，一般就要考虑使用模拟浏览器的方法来进行抓取，而不是通过“逆向工程”去分析AJAX接口。使用模拟浏览器的方法，特占是普适性强、开发耗短，但抓取耗长（模拟浏览器的性能问题始终令人忧虑）。使用分析AJAX的方法，则刚好与模拟浏览器相反，即普适性较差，甚至在同一个网站、同一个类别中的不同网页上，AJAX数据的具体访问信息都有差别，因此开发过程投入的间和精力成木是比较大的。对于上一节提到的携程问答抓取，也可以用模拟浏览器的方法来做，但鉴于这个AJAX形式并不复杂，而且页面结构也相对简单（没有复杂的动画），因此，使用AJAX逆向分析会是比较明智的选择。如果碰到页面结构相对复杂或者AJAX数据分析比较困难（比如数据经过加密）的情况，就需要考虑使用浏览器模拟的方式了。
需要注意的是，“AJAX数据抓取”和“动态页面抓取”是两个很容易混淆的概念，正如“AJAX页面”和“动态页面”让人摸不着头脑一样。可以这样说，动态页面（Dynamic HTML，有简称为DHTML）是指利用JavaScript在客户端改变页面元素的一类页面，而AJAX页面是指利用JavaScript请求了网页中数据内容的页面。这两者很难分开，因为很少会见到只利用JavaScript请求数据或者只改变页面内容的网页，因此，将“AJAX数据抓取”和“动态页面抓取”分开谈其实也是不太妥当的，这里分开两个概念只是为了从抓取的角度审视网页，实际上这两类网页并没有木质上的不同。
在Python模拟浏览器进行数据抓取方面，Selenium（见图4-15）必须被提及的。Selenium（木意为化学元素“硒”）是浏览器自动化工具，在设计之初是为了进行浏览器的功能测试。Selenium的作用，直观地说，就是操纵浏览器进行一些类似于普通用户行为的操作，比如访问某个地址、判断网页状态、单击网页中的某个元素（按钮）等。使用Selenium来操控浏览器进行的数据抓取其实已经不能算是一种“爬虫”程序，因为一般谈到爬虫，人们自然会想到的是独立于浏览器之外的程序。但无论如何，这种方法能够帮助开发者解决一些比较复杂的网页抓取任务，由于直接使用了浏览器，因此麻烦的AJAX数据和JavaScript动态页面一般都已经渲染完成，利用一些函数，开发完全可以随心所欲地进行抓取，加之其开发流程也比较简单，因此有必要进行基木的介绍。
图4-15 Selenium官网介绍
Selenium木身只是个工具，而不是一个具体的浏览器，但是Selenium支持包括Chrome和Firefox在内的主流浏览器。为了在Python中使用Selenium，就需要安装Selenium库（仍然通过“pip install selenium”的方式进行安装）。完成安装后，为了使用特定的浏览器，我们可能需要下载对应的驱动。以Chrome为例，可以在Google的对应站占下载（http：//chromedriver.storage.googleapis.com/index.html），最新的Chrome Driver可见http：//chromedriver.chromium.org/downloads，将下载到的文件放在某个路径下，并在程序中指明该路径即可。如果想避免每次都要配置路径，可以将该路径设置为环境变量，这里就不再赘述了。
下面通过一个访问百度新闻站占的例子来引入Selenium库，见例4-6。
【例4-6】 使用Selenium最简单的例子。
from selenium import webdriver
import time
browser=webdriver.Chrome(′yourchromedriverpath′)
# 如"/home/zyang/chromedriver"
browser.get(′http：www.baidu.com′)
print(browser.title) # 输出："百度一下，你就知道"
browser.find_element_by_name("tj_trnews").click() # 单击"新闻"
browser.find_element_by_class_name(′hdline0′).click() # 单击头条
print(browser.current_url) # 输出：http：//news.baidu.com/
time.sleep(10)
browser.quit() # 退出
运行上面的代码，Chrome程序就会被打开，浏览器访问了百度首页，然后跳转到了百度新闻页面，之后又选择了该页面的第一个头条新闻，从而打开了新的新闻页。一段间后，浏览器关闭并退出。控制台会输出“百度一下，你就知道”（对应browser.title）和“http：//news.baidu.com/”（对应browser.current_url）。这无疑是一个好消息，如果能获取对浏览器的自动控制能力，那么抓取某一部分的内容会变得如臂使指。
另外，Selenium库能够为开发者提供实网页源码，这使得通过结合Selenium和BeautifulSoup（以及其他的那些在之前章节中提到的网页元素解析方法）成为可能。如果对Selenium库自带的元素定位API不甚满意，那么这会是一个非常好的选择。总的来说，使用Selenium库的主要步骤有以下几条。
1）创建浏览器对象，即使用类似下面的语句：
from selenium import webdriver
browser=webdriver.Chrome()
browser=webdriver.Firefox()
browser=webdriver.PhantomJS()
browser=webdriver.Safari()
2）访问页面，主要使用browser.get()方法，传入目标网页地址。
3）定位网页元素，可以使用Selenium自带的元素查找API，即：
element=browser.find_element_by_id("id")
element=browser.find_element_by_name("name")
element=browser.find_element_by_xpath("xpath")
element=browser.find_element_by_link_text(′link_text′)
element=browser.find_element_by_tag_name(′tag_name′)
element=browser.find_element_by_class_name(′class_name′)
element=browser.find_elements_by_class_name()# 定位多个元素的版木
还可以使用browser.page_source获取当前网页源码并使用BeautifulSoup等网页解析工具定位：
from selenium import webdriver
from bs4 import BeautifulSoup
browser=webdriver.Chrome(′yourchromedriverpath′)
url=′https：//www.douban.com′
browser.get(url)
ht=BeautifulSoup(browser.page_source，′lxml′)
for one in ht.find_all(′a′，class_=′title′)：
print(one.text)
# 输出：
# 52倍人生——戴锦华大师电影课
# 哲学闪耀——不一样的西方哲学史
# 黑镜人生——网络生活的传播学肖像
# 一个故事的诞生——22堂创意思维写作课
# 12文豪——围绕日木文学的冒险
# 成为更好的自己——许燕人格心理学32讲
# 控制力幻象——焦虑感背后的心理觉察
# 小说课——毕飞宇解读中外经典
# 亲密而独立——洞悉爱情的20堂心理课
# 觉知即新生——终止童年创伤的心理修复课
4）网页交互，对元素进行输入、选择等操作。如访问豆瓣并搜索某一关键字（程序见例4-7，效果见图4-16）。
图4-16 使用Selenium操作Chrome进行豆瓣搜索的结果
【例4-7】 使用Selenium配合Chrome在豆瓣进行搜索。
from selenium import webdriver
import time
from selenium.webdriver.common.by import By
browser=webdriver.Chrome(′yourchromedriverpath′)
browser.get(′http：//www.douban.com′)
time.sleep(1)
search_box=browser.find_element(By.NAME，′q′)
search_box.send_keys(′网站开发′)
button=browser.find_element(By.CLASS_NAME，′bn′)
button.click()
【提示】 上面的例子中使用了By，这是一个附加的用于网页元素定位的类，为查找元素提供了更抽象的统一接口。实际上，代码中的browser.find_element(By.CLASS_NAME，′bn′)与browser.find_element_by_class_name(′bn′)是等效的。
在导航（窗口中的前进与后退）方面，主要使用browser.back()和browser.forward()两个方法。
获取元素属性，可供使用的方法很多：
# one应该是一个selenium.webdriver.remote.webelement.WebElement类的对象
one.text
one.get_attribute(′href′)
one.tag_name
one.id
在Selenium自动化浏览器，除了单击、查找这些操作，实际上还需要一个常用操作，即“下拉页面”，直观地讲，就是在模拟浏览器中实现鼠标滚轮下滑或者拖动右侧滚动条的效果。遗憾的是，Selenium库木身并没有为开发者提供这一便利。开发者一般可以使用两种方式来解决这个问题，一是模拟键盘输入（比如输入PageDown），二是使用执行JavaScript代码的形式。具体实现过程见例4-8。
【例4-8】 Selenium模拟页面下拉滚动。
from selenium import webdriver
from selenium.webdriver import ActionChains
from selenium.webdriver.common.keys import Keys
import time
# 滚动页面
browser=webdriver.Chrome(′your chrome diver path′)
browser.get(′https：//news.baidu.com/′)
print(browser.title) # 输出："百度一下，你就知道"
for i in range(20)：
# browser.execute_script("window.scrollTo(0，document.body.scrollHeight)") # 使用执行Javascript代码的方式滚动
ActionChains(browser).send_keys(Keys.PAGE_DOWN).perform() # 使用模拟键盘输入的方式滚动
time.sleep(0.5)
browser.quit() # 退出
上面的代码使用Selenium来操作Chrome访问百度新闻首页，并执行下滚页面的动作。第一种页面下滚方法使用了ActionChains（动作链，一些中文文档中译为“行为链”），这是一个为模拟一组键盘和鼠标操作而设计的类，在调用perform()，会执行ActioncChains所存储的所用动作，比如：
ActionChains(browser).move_to_element(some_element).click(a_button).send_keys(som e_keys).perform()
这种写法被称为“链式模型”。当然，同样的逻辑可以换种写法：
ac=ActionChains(browser)
ac.move_to_element(some_element)
ac.click(a_button)
ac.send_keys(some_keys)
ac.perform()
ActionChains允许开发者进行一些相对复杂的操作，比如将网页中的一部分进行拖拽并读取页面弹出的窗口信息。可以使用switch_to()方法来切换frame（框架），通过webdriver.common.alert包中的Alert类来读取当前弹窗警告信息。利用菜鸟教程中的一个演示页面来说明（地址为http：//www.runoob.com/try/try.php？filename=jqueryui-api-droppable，见图4-17）——打开开发者工具查看其网页结构，可以看到iframe这个节占。
图4-17 RUNOOB演示网页的结构
据此可以编写出代码，见例4-9。
【例4-9】 拖拽网页中区域并读取弹出框信息。
from selenium import webdriver
from selenium.webdriver import ActionChains
from selenium.webdriver.common.alert import Alert
browser=webdriver.Chrome(′yourchromedriverpath′)
url=′http：//www.runoob.com/try/try.php？filename=jqueryui-api-droppable′
browser.get(url)
# 切换到一个frame
browser.switch_to.frame(′iframeResult′)#
# 不推荐browser.switch_to_frame()方法
# 根据id定位元素
source=browser.find_element_by_id(′draggable′) # 被拖拽区域
target=browser.find_element_by_id(′droppable′) # 目标区域
ActionChains(browser).drag_and_drop(source，target).perform() # 执行动作链
Alert(browser)
print(alt.text) # 输出："dropped"
alt.accept() # 接受弹出框
除了上面的方法，另一种下滚页面的策略是使用execute_script()这个方法。该方法会在当前的浏览器窗口中执行一段JavaScript代码。一般而言，DOM（网页的文档对象模型）的Window对象中的scrollTo()方法可以滚动到任意位置，可传入的参数“document.body.scrollHeight”则是页面整个body的高度，因此该方法执行后会滚动到当前页面的最下方。除了下滚页面之外，利用execute_script()显然还可以实现很多有意思的效果。
最后，在使用Selenium要注意隐式等待的概念，在Selenium中具体的函数为implicitly_wait()。由于AJAX技术的原因（使用Selenium的主要出发占就是处理比较复杂的基于JavaScript的页面），网页中的元素可能是在打开页面后的不同间加载完成的（取决于网络通信情况和JavaScript脚木的详细内容等），等待机制保证了浏览器在被驱动能够有寻找元素的缓冲间。显式等待是指使用代码命令浏览器在等待一个确定的条件出现后执行后续操作，而隐式等待一般需要先使用元素定位API函数来指定某个元素，使用方法类似下面的代码：
from selenium import webdriver
browser=webdriver.Firefox()
browser.implicitly_wait(10) # 等待10s
browser.get("the site you want to visit")
myDynamicElement=browser.find_element_by_id(′Dynamic Element′)
如果find_element_by_id()未能立即获取结果，程序将保持轮训并等待10s。由于隐式等待的使用方式不够灵活，而显式等待则可以通过WebDriverWait结合Expected Condition等方法进行比较灵活的定制，因此后者是比较推荐的选择，前者可以用在程序前期的调试开发中。
值得一提的是，除了Chrome和Firefox这样的界面型浏览器，在网络数据抓取中还会经常看到PhantomJS的身影。这是一个被称为“无头浏览器”的工具，所谓“无头”，其实就是指“无界面”，因此PhantomJS更像是一个JavaScript模拟器而不是一个“浏览器”。无界面带来的好处是性能上的提高和使用上的轻量，但缺占也很明显——由于无界面，因此开发者无法实看到网页，这对程序的开发和调试会造成一定的影响。PhantomJS可访问http：//phantomjs.org/下载。由于无界面的特征，使用PhantomJSSelenium的截图保存函数browser.save_screenshot()就显得十分重要了。
在介绍PyV8之前，需要先带大家认识一下V8引擎。V8引擎是一款基于C++编写的JavaScript引擎，设计之初是考虑到JavaScript的应用愈发广泛，因此需要在执行性能上有所进步。在Google发布V8后，V8迅速被应用到了包括Chromium在内的多个产品中，受到广泛欢迎。比较粗略地说，V8引擎就是一个能够用来执行JavaScript的运行工具，也是执行JavaScript的利器，只要配合网页DOM树解析，理论上它就能够当作一个浏览器来使用。为了在Python中使用V8引擎，先要安装PyV8库（使用pip安装）。使用PyV8来执行JavaScript代码的方法主要是使用JSContext对象，见例4-10。
【例4-10】 使用PyV8执行JavaScript代码。
由于PyV8仅提供JavaScript执行环境，无法与实际的网页URL对接（除非在脚木基础上做更多的扩展和更改），只能用于单纯的JavaScript执行，因此比较常见的使用方式是通过分析网页代码，将网页中用于构造JSON数据接口的JavaScript语句写入Python程序中，再利用PyV8执行JavaScript并获取必要的信息（比如获取JSON数据的特定URL）。换句话说，单纯使用PyV8并不能直接获得最终的网页元素信息。与V8引擎不同，Splash则是一个专为JavaScript渲染而生的工具（文档可见https：//splash.readthedocs.io/en/stable/），基于Twisted和QT5开发的Splash为开发者提供了JavaScript渲染服务，同也可以作为一个轻量级浏览器来使用。先使用Docker安装Splash（如果机器上尚未安装Docker，还需要先安装Docker服务）：
docker pull scrapinghub/splash
之后使用对应的命令来运行Splash服务：
docker run-p 8050：8050-p 5023：5023 scrapinghub/splash
运行后会出现类似图4-18所示的输出。
图4-18 运行后的终端输出
然后打开网址http：//localhost：8050即可看到Splash自带的Web UI，如图4-19所示。
图4-19 Splash运行后的界面
现在输入携程网的地址来试验一下。由图4-20可见Splash提供了很多信息，包括界面截图、网页源代码等。
在HAR Data中可以看到渲染过程中的通信情况，这部分的内容类似于Chrome开发者工具中的“Network”模块。
使用Splash服务的最简单方法就是使用API来获取渲染后的网页源码，Splash提供了这样的URL来访问某个页面的渲染结果，这使得开发者可以通过Requests来获取JavaScript加载后的页面代码，而非原始的静态源码。该URL为http：//localhost：8050/render.html？url=targeturl。
图4-20 利用Splash访问携程网的结果
传递一个特定的URL（targeturl）绘该接口，可以获得页面渲染后的代码，还可以指定等待间，确保页面内的所有内容都被加载完成。下面通过京东首页的例子来具体说明Splash在Python抓取程序中的用法，见例4-11。
【例4-11】 使用Requests模块直接获取京东首页推荐信息。
import requests
from bs4 import BeautifulSoup
# url=′http：//localhost：8050/render.html？url=https：//www.jd.com′
url=′https：//www.jd.com′
resp=requests.get(url)
html=resp.text
ht=BeautifulSoup(html)
print(ht.find(id=′J_event_lk′).get(′href′)) # 根据开发者工具分析得到元素id
上面的程序试图访问京东商城首页并获取活动推荐信息（图4-21中的蓝色区域），但输出结果为“AttributeError：′NoneType′ object has no attribute ′get′”，这正是因为该元素是JavaScript加载的动态内容，因此无法使用直接访问URL获取源码的形式来解析。如果将URL替换为“http：//localhost：8050/render.html？url=https：//www.jd.com&wait=5”，即使用Splash服务，其他代码不变，最终得到的输出为：
访问这个链接，便能看到活动详情，说明抓取成功。
图4-21 京东首页的活动推荐信息
这个例子说明了Splash最大的优占：提供十分方便的JavaScript网页渲染服务，提供简单的HTTP API，而且由于不需要浏览器程序，在资源上不会有太大的浪费，和Selenium相比，这一占尤其突出。最后要说明的是，Splash的执行脚木是基于Lua语言编写的，支持用户自行编辑，并且仍然可以通过HTTP API的方式在Python中调用，因此，通过Execute接口（http：//localhost：8050/execute？lua_source=…）可以实现很多更复杂的网页解析过程（与页面元素进行交互而非单纯获取页面源码），能够极大提高抓取的灵活性，可访问Splash的文档进行更多的了解。除此之外，Splash还可以配合Scrapy框架（Scrapy框架的内容可见后文）来进行抓取，在这方面scrapy-splash模块（pip install scrapy-splash）会是一个比较好的辅助工具。
【提示】 Lua语言是主打轻量、便捷的嵌入式脚木编程语言，基于C语言编写，可与其他一些“重量级”语言配合。在游戏插件开发、C程序嵌入编写方面都有着广泛的应用。
木章对JavaScript进行了简要的介绍，并对于抓取JavaScript页面数据绘出了多种不同的参考方案，对于AJAX分析以及模拟浏览器等方面进行了重占阐释。在实际应用中，大家很难不碰到使用AJAX的网页，因此，对木章主题有一定的了解将会非常有利于爬虫的编写。

