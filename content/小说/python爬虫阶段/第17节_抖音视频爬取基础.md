---
title: "第17节_抖音视频爬取基础"
slug: "第17节_抖音视频爬取基础"
date: "2025-04-25T00:37:01.899870+00:00"
---

# 第17节：抖音视频爬取基础

## 学习目标

- **<font color="red">掌握抖音平台的反爬机制特点</font>**
- **<font color="blue">学习使用requests和正则表达式获取视频基本信息</font>**
- **<font color="green">理解抖音视频页面的HTML结构</font>**
- **<font color="purple">熟练分析和定位关键信息位置</font>**
- **<font color="orange">掌握绕过基本反爬措施的技巧</font>**

## 知识点

### 抖音平台反爬机制

- **<font color="red">基本反爬策略</font>**：
  - User-Agent检测
  - IP频率限制
  - 登录验证
  - 动态加载内容
  - 加密参数

- **<font color="blue">反爬特点</font>**：
  - 多层次防护
  - 频繁更新算法
  - 行为分析识别
  - 验证码机制

- **<font color="green">应对策略</font>**：
  - 模拟真实浏览器环境
  - 控制请求频率
  - 使用代理IP池
  - 分析加密参数
  - 保持Cookie会话

### 抖音视频页面分析

- **<font color="red">页面结构特点</font>**：
  - 动态加载内容
  - 懒加载机制
  - 数据存储在JavaScript变量中
  - XHR请求获取数据

- **<font color="blue">关键信息位置</font>**：
  - 视频标题：HTML标签或JS变量
  - 作者信息：用户主页链接
  - 点赞数据：动态加载
  - 评论信息：异步请求
  - 视频链接：加密或嵌入在JS中

- **<font color="green">分析工具</font>**：
  - 浏览器开发者工具
  - Network面板监控请求
  - Elements面板分析结构
  - Console执行JavaScript

### 使用requests和正则表达式爬取

- **<font color="red">构造请求</font>**：
  - 设置合适的Headers
  - 维护Cookie信息
  - 处理重定向
  - 模拟移动设备

- **<font color="blue">提取视频信息</font>**：
  - 标题提取正则：`<title>([^<]+)</title>`
  - 作者信息正则：`"author":\s*{([^}]+)}`
  - 视频链接正则：`"playAddr":\s*"([^"]+)"`
  - 点赞数正则：`"diggCount":\s*(\d+)`

- **<font color="green">数据处理</font>**：
  - JSON数据解析
  - 字符串清洗
  - URL解码
  - 数据结构化存储

## 典型示例

### 基本请求与页面获取

```python
import requests
import re
import time
import random

# 设置请求头
headers = {
    'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.3 Mobile/15E148 Safari/604.1',
    'Referer': 'https://www.douyin.com/',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
}

# 抖音视频页面URL
video_url = "https://www.douyin.com/video/7123456789012345678"

# 发送请求
try:
    response = requests.get(video_url, headers=headers, timeout=10)
    if response.status_code == 200:
        print("页面获取成功！")
        html_content = response.text
    else:
        print(f"请求失败，状态码：{response.status_code}")
        html_content = ""
except Exception as e:
    print(f"请求异常：{e}")
    html_content = ""

# 随机延时，模拟人类行为
time.sleep(random.uniform(1, 3))
```

### 提取视频基本信息

```python
# 提取视频标题
def extract_title(html):
    title_pattern = r'<title[^>]*>([^<]+)</title>'
    title_match = re.search(title_pattern, html)
    if title_match:
        title = title_match.group(1)
        # 清洗标题，去除平台后缀
        title = re.sub(r'- 抖音$', '', title).strip()
        return title
    return "未找到标题"

# 提取作者信息
def extract_author(html):
    # 方法1：从HTML中提取
    author_pattern = r'"nickname":\s*"([^"]+)"'
    author_match = re.search(author_pattern, html)
    if author_match:
        return author_match.group(1)
    
    # 方法2：从页面元素提取
    author_element_pattern = r'<span[^>]*class="[^"]*author[^"]*"[^>]*>([^<]+)</span>'
    author_element_match = re.search(author_element_pattern, html)
    if author_element_match:
        return author_element_match.group(1)
    
    return "未找到作者信息"

# 提取点赞数
def extract_like_count(html):
    like_pattern = r'"diggCount":\s*(\d+)'
    like_match = re.search(like_pattern, html)
    if like_match:
        return like_match.group(1)
    return "未找到点赞数"

# 提取评论数
def extract_comment_count(html):
    comment_pattern = r'"commentCount":\s*(\d+)'
    comment_match = re.search(comment_pattern, html)
    if comment_match:
        return comment_match.group(1)
    return "未找到评论数"

# 提取视频链接
def extract_video_url(html):
    video_pattern = r'"playAddr":\s*"([^"]+)"'
    video_match = re.search(video_pattern, html)
    if video_match:
        video_url = video_match.group(1)
        # URL解码
        video_url = video_url.replace('\\u002F', '/')
        return video_url
    return "未找到视频链接"

# 提取视频封面
def extract_cover_url(html):
    cover_pattern = r'"cover":\s*"([^"]+)"'
    cover_match = re.search(cover_pattern, html)
    if cover_match:
        cover_url = cover_match.group(1)
        # URL解码
        cover_url = cover_url.replace('\\u002F', '/')
        return cover_url
    return "未找到封面链接"

# 提取所有信息
def extract_all_info(html):
    if not html:
        return {}
    
    info = {
        "标题": extract_title(html),
        "作者": extract_author(html),
        "点赞数": extract_like_count(html),
        "评论数": extract_comment_count(html),
        "视频链接": extract_video_url(html),
        "封面链接": extract_cover_url(html)
    }
    return info

# 获取并打印视频信息
if html_content:
    video_info = extract_all_info(html_content)
    for key, value in video_info.items():
        print(f"{key}: {value}")
```

### 处理加密参数和动态内容

```python
import json

# 从JavaScript变量中提取数据
def extract_from_js_variable(html):
    # 寻找包含视频信息的JavaScript变量
    js_data_pattern = r'window\.__INIT_PROPS__\s*=\s*(\{.+?\});'
    js_data_match = re.search(js_data_pattern, html, re.DOTALL)
    
    if not js_data_match:
        return {}
    
    try:
        # 提取JSON字符串并解析
        js_data_str = js_data_match.group(1)
        # 处理转义字符
        js_data_str = js_data_str.replace('\\"', '"')
        # 尝试解析JSON
        data = json.loads(js_data_str)
        
        # 从复杂的JSON结构中提取视频信息
        # 注意：实际结构可能更复杂，需要根据实际情况调整
        video_data = {}
        if 'videoData' in data:
            video_data = data['videoData']
        elif 'initialData' in data and 'videoInfo' in data['initialData']:
            video_data = data['initialData']['videoInfo']
            
        return video_data
    except json.JSONDecodeError as e:
        print(f"JSON解析错误：{e}")
        return {}
    except Exception as e:
        print(f"提取数据异常：{e}")
        return {}

# 提取并处理数据
js_data = extract_from_js_variable(html_content)
if js_data:
    print("\n从JavaScript变量中提取的数据:")
    # 打印部分关键信息
    keys_to_show = ['id', 'desc', 'createTime', 'author', 'music', 'stats']
    for key in keys_to_show:
        if key in js_data:
            print(f"{key}: {js_data[key]}")
```

## 实际示例

### 批量爬取抖音热门视频信息

```python
import requests
import re
import json
import time
import random
import csv
from urllib.parse import quote

# 设置请求头
headers = {
    'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.3 Mobile/15E148 Safari/604.1',
    'Referer': 'https://www.douyin.com/',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
}

# 搜索关键词
keyword = "美食"
encoded_keyword = quote(keyword)

# 搜索URL
search_url = f"https://www.douyin.com/search/{encoded_keyword}"

# 发送请求获取搜索页面
try:
    response = requests.get(search_url, headers=headers, timeout=10)
    if response.status_code == 200:
        search_html = response.text
        print("搜索页面获取成功！")
    else:
        print(f"搜索请求失败，状态码：{response.status_code}")
        search_html = ""
except Exception as e:
    print(f"搜索请求异常：{e}")
    search_html = ""

# 提取视频链接
def extract_video_links(html):
    # 提取视频ID
    video_id_pattern = r'"id":\s*"(\d+)"'
    video_ids = re.findall(video_id_pattern, html)
    
    # 构建视频链接
    video_links = [f"https://www.douyin.com/video/{video_id}" for video_id in video_ids]
    return list(set(video_links))  # 去重

# 提取视频信息
def get_video_info(video_url):
    try:
        # 随机延时
        time.sleep(random.uniform(1, 3))
        
        # 发送请求
        response = requests.get(video_url, headers=headers, timeout=10)
        if response.status_code != 200:
            print(f"获取视频页面失败，状态码：{response.status_code}")
            return {}
            
        html = response.text
        
        # 提取信息
        title = extract_title(html)
        author = extract_author(html)
        like_count = extract_like_count(html)
        comment_count = extract_comment_count(html)
        
        return {
            "视频链接": video_url,
            "标题": title,
            "作者": author,
            "点赞数": like_count,
            "评论数": comment_count
        }
    except Exception as e:
        print(f"处理视频 {video_url} 时出错：{e}")
        return {}

# 提取标题函数
def extract_title(html):
    title_pattern = r'<title[^>]*>([^<]+)</title>'
    title_match = re.search(title_pattern, html)
    if title_match:
        title = title_match.group(1)
        # 清洗标题，去除平台后缀
        title = re.sub(r'- 抖音$', '', title).strip()
        return title
    return "未找到标题"

# 提取作者信息
def extract_author(html):
    author_pattern = r'"nickname":\s*"([^"]+)"'
    author_match = re.search(author_pattern, html)
    if author_match:
        return author_match.group(1)
    return "未找到作者信息"

# 提取点赞数
def extract_like_count(html):
    like_pattern = r'"diggCount":\s*(\d+)'
    like_match = re.search(like_pattern, html)
    if like_match:
        return like_match.group(1)
    return "0"

# 提取评论数
def extract_comment_count(html):
    comment_pattern = r'"commentCount":\s*(\d+)'
    comment_match = re.search(comment_pattern, html)
    if comment_match:
        return comment_match.group(1)
    return "0"

# 主函数
def main():
    # 提取视频链接
    if not search_html:
        print("搜索页面为空，无法提取视频链接")
        return
        
    video_links = extract_video_links(search_html)
    print(f"找到 {len(video_links)} 个视频链接")
    
    # 限制爬取数量
    max_videos = 5
    video_links = video_links[:max_videos]
    
    # 存储结果
    results = []
    
    # 爬取每个视频的信息
    for i, link in enumerate(video_links):
        print(f"正在处理第 {i+1}/{len(video_links)} 个视频...")
        info = get_video_info(link)
        if info:
            results.append(info)
    
    # 保存为CSV
    if results:
        with open('douyin_videos.csv', 'w', newline='', encoding='utf-8-sig') as f:
            fieldnames = results[0].keys()
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(results)
        print(f"成功保存 {len(results)} 条视频信息到 douyin_videos.csv")

# 执行主函数
if __name__ == "__main__":
    main()
```

### 提取抖音视频评论

```python
import requests
import re
import json
import time
import random

# 设置请求头
headers = {
    'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.3 Mobile/15E148 Safari/604.1',
    'Referer': 'https://www.douyin.com/',
    'Accept': 'application/json, text/plain, */*',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Content-Type': 'application/json'
}

# 视频ID
video_id = "7123456789012345678"  # 替换为实际视频ID

# 提取评论需要的参数
def extract_comment_params(video_url):
    try:
        response = requests.get(video_url, headers=headers, timeout=10)
        if response.status_code != 200:
            print(f"获取视频页面失败，状态码：{response.status_code}")
            return None
            
        html = response.text
        
        # 提取aid参数
        aid_pattern = r'"aid":\s*"(\d+)"'
        aid_match = re.search(aid_pattern, html)
        aid = aid_match.group(1) if aid_match else ""
        
        # 提取aweme_id参数
        aweme_id_pattern = r'"awemeId":\s*"([^"]+)"'
        aweme_id_match = re.search(aweme_id_pattern, html)
        aweme_id = aweme_id_match.group(1) if aweme_id_match else video_id
        
        # 提取cookie中的必要参数
        cookies = response.cookies.get_dict()
        
        return {
            "aid": aid,
            "aweme_id": aweme_id,
            "cookies": cookies
        }
    except Exception as e:
        print(f"提取评论参数失败：{e}")
        return None

# 获取评论
def get_comments(video_url, cursor=0, count=20):
    # 获取必要参数
    params = extract_comment_params(video_url)
    if not params:
        return []
    
    # 构造评论API URL
    comment_api_url = "https://www.douyin.com/aweme/v1/web/comment/list/"
    
    # 请求参数
    query_params = {
        "aid": params["aid"],
        "aweme_id": params["aweme_id"],
        "cursor": cursor,
        "count": count,
        "device_platform": "webapp"
    }
    
    try:
        # 设置cookies
        cookies = params["cookies"]
        
        # 发送请求
        response = requests.get(
            comment_api_url, 
            params=query_params, 
            headers=headers, 
            cookies=cookies,
            timeout=10
        )
        
        if response.status_code != 200:
            print(f"获取评论失败，状态码：{response.status_code}")
            return []
        
        # 解析JSON响应
        data = response.json()
        
        # 提取评论
        comments = []
        if "comments" in data:
            for comment in data["comments"]:
                comment_info = {
                    "用户名": comment.get("user", {}).get("nickname", "未知用户"),
                    "内容": comment.get("text", ""),
                    "点赞数": comment.get("digg_count", 0),
                    "时间": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(comment.get("create_time", 0)))
                }
                comments.append(comment_info)
        
        return comments
    except Exception as e:
        print(f"获取评论异常：{e}")
        return []

# 主函数
def main():
    video_url = f"https://www.douyin.com/video/{video_id}"
    
    # 获取第一页评论
    comments = get_comments(video_url)
    
    if comments:
        print(f"成功获取 {len(comments)} 条评论：")
        for i, comment in enumerate(comments):
            print(f"\n评论 {i+1}:")
            for key, value in comment.items():
                print(f"{key}: {value}")
    else:
        print("未获取到评论")

# 执行主函数
if __name__ == "__main__":
    main()
```

## 思考题

1. 抖音平台的反爬机制与普通网站相比有哪些特殊之处？如何有效应对这些特殊的反爬措施？
2. 为什么抖音视频的真实播放地址难以直接通过HTML源码获取？有哪些方法可以解决这个问题？
3. 在爬取抖音视频时，如何判断是否需要登录才能获取完整数据？如何模拟登录状态？
4. 抖音平台经常更新其网页结构和API，如何设计一个可维护的爬虫程序来应对这些变化？
5. 从法律和道德角度考虑，爬取抖音视频内容有哪些限制和注意事项？如何在合法合规的前提下进行数据采集？
  
## 小结

- **<font color="red">抖音平台采用多层次反爬机制，需要综合应对策略</font>**
- **<font color="blue">页面分析是爬取抖音视频的关键步骤，需要掌握HTML结构和JavaScript数据提取</font>**
- **<font color="green">正则表达式是提取抖音视频基本信息的有效工具</font>**
- **<font color="purple">请求头设置和Cookie管理对成功爬取至关重要</font>**
- **<font color="orange">批量爬取需要控制频率和使用代理IP，避免被封禁</font>**

## 总结

本节课介绍了抖音视频爬取的基础知识和技术方法。抖音作为当前最流行的短视频平台之一，其反爬机制相对复杂，需要我们掌握更多的网络爬虫技巧。通过学习抖音平台的反爬特点、页面结构分析方法以及使用requests和正则表达式提取视频信息的技术，我们可以实现对抖音视频基本信息的采集。

在实际爬取过程中，我们需要注意模拟真实用户行为、控制请求频率、处理加密参数等问题。同时，也要关注法律法规和平台规则，确保爬虫行为合法合规。随着抖音平台的不断更新，爬虫技术也需要持续学习和调整，以适应新的变化。下一节课，我们将进一步学习如何使用更高级的技术获取抖音视频的真实地址并下载视频内容。